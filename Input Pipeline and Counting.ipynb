{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regresion for counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librarys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the images the function input_pipeline is going to create a pipeline where tensorflow can read images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow.contrib.keras as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 10\n",
    "IMA_HEIGHT = 64\n",
    "#IMA_WIDTH = int(round(IMA_HEIGHT*1.5,0))\n",
    "IMA_WIDTH = 64\n",
    "IMA_DEPTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(paths,batch_size=2):\n",
    "    file_names = tf.train.match_filenames_once(paths)\n",
    "    filename_queues = tf.train.string_input_producer(file_names)\n",
    "    \n",
    "    reader = tf.WholeFileReader(name=\"reader\")\n",
    "    key, value = reader.read(filename_queues)\n",
    "    image = tf.image.decode_jpeg(value)\n",
    "    image = tf.image.resize_images(image,[IMA_HEIGHT,IMA_WIDTH],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    image.set_shape((IMA_HEIGHT,IMA_WIDTH,IMA_DEPTH))\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "    batch_size = batch_size\n",
    "    num_process_threads = 2\n",
    "    min_after_dequeue = 1\n",
    "    \n",
    "    labels , images = tf.train.shuffle_batch(\n",
    "        [key,image],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_process_threads,\n",
    "        capacity= min_after_dequeue + (3)* batch_size,\n",
    "        min_after_dequeue=min_after_dequeue,name=\"Batch\"\n",
    "    )\n",
    "    return labels, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pixel_normalization(image,maxp=255,minp=0):\n",
    "    return(image - minp)/(maxp - minp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\",index_col=\"train_id\")\n",
    "scaler = preprocessing.StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "text2Num = lambda label:np.array([data[int(str(text)[10:12])] for text in label],ndmin=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "y_pipe, x_pipe = input_pipeline(\"./Train/*.jpg\",BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    try:\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            batch_targets, batch_features = sess.run([y_pipe,x_pipe])\n",
    "            batch_targets = text2Num(batch_targets)\n",
    "            batch_features =  pixel_normalization(batch_features)\n",
    "            \n",
    "            X.append(batch_features)\n",
    "            Y.append(batch_targets)\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done Reading')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "    coord.join(threads)\n",
    "X = np.vstack(X)\n",
    "Y = np.vstack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.,   4.,  10.,   1.,   0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvXm0XNV1Jv6dmuuNkp7m8WlASIwCxCgGGczkGLCx4xg7\nDk4Tk6nTdjrpxO5eSXf6l+SXZGXyLysrv9B2bCexATsOBjMLMU8CgRg0IDTPs95cc9XpP6p097c3\neujZQAmnzreWlk69c+rWqXPvrbv3+fb+tvPeIyAgoLUQO9kTCAgIaD7CjR8Q0IIIN35AQAsi3PgB\nAS2IcOMHBLQgwo0fENCCCDd+QEAL4j3d+M6565xzG51zm51zX32/JhUQEPDBwv20ATzOuTiAtwFc\nDWA3gJcB3OK9X//+TS8gIOCDQOI9vPcCAJu991sBwDl3F4CbAIx642eybb6jq/v4nU6a9sfIOXfc\ngQ4a/rijTtRrfvicmsio8+AeazZV6X0x986ZjPLJo3Z6M5KPWbPTH+Vw7zKNd7zL+9q7DeaJyRFi\nehVqtUrUjjnpc2Yiev5mvUd55Wtmfu/23UY97aOv/rs9DJ1dKydjU8lU1B4a7FPjahWZc7ajQ/WV\ni8Wonc5konZbWt+eiUQyahfyI/r4rj62f2AAuVz+Xc828N5u/BkAdtHr3QAufLc3dHR144bPf7Hx\nSs8tRhdOsVpQffG4fOFETKYcM8eo0Vl29oaIycI7xKO2R1l/Viwdtas13Zdw8tk1J8fLuLgaN1Sm\nExnXSxyjzy6bSTpP36dWlXHVqhrXlpFj5sv6GKOd0ETM3EZxuhm9vmlL5Zy88HREV1HjfFmOmW7L\nqr7hkf6onUm0ybiU/qx8lc+ZvqHj/ENL7dJwTo1zSXoY2F+4Ip33BP1gVvR3idG1U67qPtTkffG4\nPtcVJ+dm9oyZUfvpFT9Q4wYPy5zPvOQK1Xdwx5aoPWf+wqh93vyJalzP5OlRe/O651XfcHwSAOD/\nfOtfMRZ84Jt7zrnbnXOrnXOrC/ncid8QEBDwgeO9PPH3AJhFr2c2/qbgvb8DwB0A0DNlqq82ftWr\n5pc5QQ+uhJkWPwkSkPY7nphVekLYJxw9NNO1o1G7kNSuR5U+K2l+F8vKopB2yXyXLJl8xaK2GlxS\n3teGlOobqZaidjzBpq0aBn5YpYyJHSfLplqRvlRGf1a+JFbJuacvVn0vr3kjamfIYvHWSmuXvnK5\npPqSKTFZ2UorVYp6XELmlU1rq2HOrNlRe+3bb0fteNJetnR9uKTqKUEeNmn6rKG8Pi9dGbH0KtBP\n9RhZfuN7Jqi+kZGhqN1/+LAcb/wcNW7h6XKrlAraol166eVR+80XVkTtf3xlSI0759wLovbUrF6r\nxXPqF0UmMbY9u/fyxH8ZwCnOubnOuRSAzwK47z0cLyAgoEn4qZ/43vuKc+4/A3gEQBzAP3nv171v\nMwsICPjA8F5MfXjvHwTw4Ps0l4CAgCbhPd34Pw2OUSXpuKWQxI/KJNOqL1+VPvYkU2bHHDHxb5hO\nAoAq+b4FdNNbtLfjabe+EjPH5911crxrxm9FXHzJtPFHK4q90H5xgvra6X0jxudM0f5FpWJ2womV\nqDg5fm5Ez7FKfvH6tRtVXywhn1318p2ZCQA062HXMUl7FKWqfHYsps9tifYanKHRBoeEsqoyfWX2\nbyp0LpzT681sQIH2IZJpc43xR5vzGW+Xsf1Hj6g+ZhEKdC5mT2hT43L0PS1duOrZF6P2adPEd+8b\n1POoDMhnv7BF04VLz6nvE8QS+loZDSFkNyCgBRFu/ICAFkTTTf1jiBtqqBwTGmakrE2cREr6UmAz\n1wRaVMT0dMZMT6YoMIfe54ePqnGuS4ImaiaAhwNKymR+p0xQCjMqpZrpo88um5/dKrknuZJ8VtXr\n71kCrYcJHiqVxJx1ZBIn4zoIiON+Rqr6e8bI7XJejl/zxj2DHDMR0+arj1GQVJFcMKfPbYzOkzP0\n7KRpPVF791Y6n5bfpHlp0hIAXTt8vaSSmvYrF+Q7x5P6KLEyUaTGvUzTYdLkPpw6/ww1btrUyVH7\n4RUrVd8pp0nQzrzxMsdn33hKjZsze0rUzrbrNSiODAAAfE2f59EQnvgBAS2IcOMHBLQgwo0fENCC\naKqP75xDMlH/rakYPy1GNIy3iS0Vptjot8rkY3jV1r9pvDeQoMSTsskWjMXls+Im/LNCvh6HxtbK\n+rsk28RHrBaNb03TipmED6aGHH2bpJkHU1RFpz87Qb51hfYG8iW9T5CkENWYCSuGo/nHpC9lPOgq\nUUcVE7JbKcjrqd2yxn0DA3oetD9Srmoff8vbkrzCCTaVmF6PGO1rlAxFytdVitbGJuJQF8pls1Zp\nCT+Om+u2UpSxI0WhHPfs3aXGbdwqIceplJ7/wAGJdN+RklDfM887X41b8dTqqP2J5QtV319++xkA\nwP7DwxgLwhM/IKAFEW78gIAWRPMj9xq/NZx9BgAUIIa4MflY7CDBUVpWqoEywlxVm2S5EpmsSbHr\nYoZuq5aIokobU5woqwSZ2yZdHvkCm4p6HmkSUyhWtImdpsi4Wo3cEa/NV/ZpEl7PMUnuA1ulMUNf\nxSiTMW76khRVOTAiX66jQ683r2nNiGN0Z9vlGEOSZVYx69E7fWrU3rJnn+obHhTT2afI1M/p7LZU\nilyQgl7TbAdlBtK5dTUT/UfRelbngZlKK4pSoSi8NJnwe/cd0McgF7VS0efTF+R79g3IWsVMFOJl\nl4jpX4B2Rz7xkbMAAHt2bsNYEJ74AQEtiHDjBwS0IJpq6ntIsow3JnacxNcSMLYzWVe885tOG9EF\nPkbC7OpThFsCHIFn2AVKUDHBaHCkHecpWszknaBQE1Ouzemd8FxRzNTObEb1Vei7VekYCSs8UZb1\nqcT0/J0XkzUe52QbPS5Fa+fNLnaOzOX2tMw/V9LnJU7RizNnaJmonQcORe0kace5kjajdx8Q8Yop\nUyapvoP7DkbtWEzmYcUm1DXRptd0aITMajqhWbOmnpieZNwcn8zquEmw6SbGIjcoZnqyoHf1C23T\novaM6dNV3+OvyW79knY5/sRpp6txm7ZvjtpTJ41TfZmGq2WDWUdDeOIHBLQgwo0fENCCCDd+QEAL\normRexCJ6pih28oUZRYra18yQVrjOmPOij+Q/1XVvmSc/MxSMS/vMVFgVdpfqJjlSZKPqLTnnXby\ns9RVNHsISaLOSnYPgfzHeEr856QR4igS3eYM1VelfY5YgrLKTGQdK68nTNZdnI6fV9SqPi8p2nzZ\nvme/6ktnOCtOztOVl+hotM3bxRc+RD49oAVHSjnZu6gZSi1JlGa+aKg+otESlKFZLJkIvxpRvOZ7\nlilb1Jtoy5EBiZQrkOCIy05V49K0d/T8Yw+rvsWnz43a4yafJn9fcrYa98qaV6P2wW1a5a6WqtOn\nI2NUsg5P/ICAFkS48QMCWhBNjtxzQKVuvllN/BhVy6nFNQVWZt03rqhiEi0SxKvVzPHLpGXOlXlq\nhrNLkHZcxbgjVTpmnKi9eEybhgWKwMtasQ2iEmslo6lGx4wRveSN6ami/wwVVyKTPs6RX95OhJKM\n7Lmg43MEZFty9GNYbYxCXuYRIxP7yRdXq3HXXrU8aq84eEj1sdviKNoyYaI+qxQ6GS8Yt4XGlssm\nGYlQJh4s3akpwQS5l+ve1hXiTp2/SMYRZRwz5a9KBXEvZy7UCTY1qjR0/wP/ErX79mktxAnd8j27\nJvWqvmrDHdzw1laMBeGJHxDQggg3fkBACyLc+AEBLYjm+vi+BufrvnbChLKyprzVHfdMX1Gobyap\nfTF+ny30zFVOk8pHNP4iZZkZNw2OqL8CCWO6kgmHJVWHQsWbPppvTK+BI1+ew0arJlux4jgrTs+x\nrU38RfZ90yY8mKNSvfF9+eM49Llm9Ox5kdvTWkd+2EvGWYb2PCp2vyJFYcU1I7JKexRdtFR9RVOP\ngPZsyiZUO1/lNaUTWrZVkonCHNGUYIXo5UXzFqk+3udgUdh8TgticBnxdeteUX3nX/DRqH3t9Z+L\n2gND/WrcJecsidpZp4//708/CQAo1cYWs3vCJ75z7p+ccwedc2vpbxOccyucc5sa/48f06cFBAR8\nKDAWU//bAK4zf/sqgJXe+1MArGy8DggI+BnBCU197/3Tzrle8+ebACxvtL8D4EkAv3/CT3MOSKSP\n21VmNQtLsZFJ7CgbrVLRJpmLy7GTpnQ1yLxPkllXNSoaVXIXijU9jwxplmfIRagamsuT/Z2yiYZs\nRsdNZh25P54ozFjSlBRn/cCs0ePzx6c7k8YCzFP2H7smAOCJVuTouZ7xOiPswH6h38rOrCOZ6TmK\naGtL6fP/2FPPRu2br7tG9T34mPT1lYQOq1b0eqRIpMObYgUd5OLkiep0CaMfSOIe1aQ+79n2jqg9\nMjKo+8i1KuVkjj1TdOTe0UMSlTiRjgdomnFoROo8tI/T7tMzq9ZE7UyXPheL58wDALyyagvGgp92\nc2+K9/6YXMp+AFPebXBAQMCHC+95V9/Xd9T8aP3Oududc6udc6sLY4wjDggI+GDx0+7qH3DOTfPe\n73POTQNwcLSB3vs7ANwBABOnTvO+sQuacHpXtUbCCPGKqfhJPyu8G11LGDOXtpmdcSkcJ6mw/pkx\nxROse2fKU1VKpIeW7pQ5xe3vp8yjbEpXcTBdzUTTxcg9YUEQmKSlCi2IMwk2lRrveMvx7W5vV7to\n4mW6s6qvMCwm6/iOrqh9dERXaFWJOMZlmtAl61OmyDcr+vHRZRdE7VMXnKL6XtsgwhO7d0oyT9qU\nLGONw5iJohzhpB3HSUvazcpQRdxiXruQVarRbHRPUCV3qneKmN/b9+qkpRhJpM+ZqQVH4nQdMzOV\ntqxPXqrl5kp6x79/sH4dVN8lOlHNZ0yj3on7ANzaaN8K4N6f8jgBAQEnAWOh8+4E8AKAU51zu51z\ntwH4MwBXO+c2Afho43VAQMDPCMayq3/LKF1Xvc9zCQgIaBKaLsQRbxgZNUMhxYqU6WXEFFmHvEqU\nV8zYK458cuvrcDZdkbLRmDarH1Ton4rxi1Ok258neile08uYIB88ZoQ+iqNlzwGIuwqNk2Ok0mYv\ngOisVMbo6pelL0cbGIsWzlHjtuzeK5+b15FwcaKo2trlO+/r1+vB1GFXVlNPfTmK3KNSW+PaO9W4\nvYdk32DKFF1e67M3SfjIX/7/34jaxYKZLwlxOFgRTaqTQFmfsbi+PnrnzI7aGzbrDLdqUejItKlB\nMDwifTtp/8KZMnA1Sl98fPXrqm/5FbNkvkQFHziQV+NmLz4zah/e9pbqS3W0Nz7X7I+NghCrHxDQ\nggg3fkBAC8LZhJgPEpOnTvefvvVLADQlBWgTOGUj1ShrJEemUHubNi85YaJU0bwLJ+mwj+DM9++g\nSLgRo9+W5t9JrhRrouJY16LktUnJ7knKmINlihTkWaVNJGOFotMqJmJO1Q+gz2IdPQAYIdPzwrPO\nVH1r3xYazRFVWTVUHGv1d2c1JTg0LDEbXF7r5muvVeNeevVVeo824X/x0zdE7Z4JQpX9j7/4GzWu\nVpU1TpispSIlUHVmxdSP24q1h4Ue87YCMZ/rkr2uKIKTXAJvk2UoAjJtIvcKOYkGrJGr1pnVn/XY\nQ49F7csvnq/6HOp05A8ffQGHjg6YsNV3IjzxAwJaEOHGDwhoQYQbPyCgBdHk2nkepUaGW81kvnHU\nqxWXqJIoZSYlvmTFONeqrLDxi2skAOFpLyCb1GGRvBfgjJtWLIvPX6GMvE5DZRWo/HW8akNq5XXB\nCGyohELKaCun9PG57FvCiGOofQLKJqyacRMycsw33tLUEJeQLtDaxwx/2kYZbgMjWhiiTFRiZ1J8\n2oP9Ouy3t7c3atuL8enVr0Xtqy45L2p/4mMfU+Pu/fF9UbtqhFW4vmK5KCe0ZGjcZBuFHxdNXQei\nnmsxvQ9RppBdPn8xc13VaD3yw0OqL00iJomMjDvar+m86bMk42/G/NNU35Ke+nl/7LlXMRaEJ35A\nQAsi3PgBAS2IJuvqS3Zd0pQp5ki1uNFNi5PpXMuI2Wj14Gtk+iedPoYzmXzHwFGBADA8TKZtRmf4\nsWZb3lA+CtXj034AkCGizhvdftbx6+qQ71k11GeNI8Sq+nvWqKRWjfT+koY67M9TGbG0qWNAZimf\ni6TVNqGS1NUBrZcXIxcnnxAXadPbWijiaL9E61135XLVN2Vij7ygTMaHH3lQjdNUpRFFKdFakXsW\nN2XaK2Syx+OjC5NUy5riZb0/T67QhIy+3g4MyTVcNYqQuUM7o/bEHpG2SJl5zF4gJbXMpYM/+M4j\nAIC9R3T042gIT/yAgBZEuPEDAloQTd7VB8oN6yVmai7FWPLaCFTUukXEN0m/Ve9I0qEKszCRU44i\n6GIkcFA15ZhqbAIn9PIUaJdclbEqmYg2MuWyptJtmcz2klEBSXKpMKrmWixoMzqZFmYjZSLy4nFJ\ngqnExCwtmkhGlr6rWglwKlfFdIsVoRgeloizTFqbtiXa4uYEqUGzo50kk/ihJ59QfZOJLbn0isuj\ntjeXbYVdQeOO8DnkdbRuC3maaM/o47NXV4hp94+rGlfycvy+vI7YZFfLCmxUU3LOHLFWaa9Lig0P\nyjH3rt2s+i69rJ4su2/fPRgLwhM/IKAFEW78gIAWRLjxAwJaEM0V4nAuioKqGf+Wy0Qls9oB47JZ\nRRJ1bDdOfp72DVLDOkKs2CHZXXGi8JyhFV1WaLSKoXy4PHOBhDjS9ueTSnvVTMkoT9sBHSZDrDAo\n4hVfvO2XovZ377xLjTvSJ5lkqbQujVUkXzVJgpqJsik7xf56Sq+3V9F/MjBvouIS9NwomuOjJr4v\n7xmYrQbkh8Tnb+vqVn2pcSL0yfUPKia0M8bzqmjf2hPd1tYh61EYHlHj2jKyjsWSpuxiFPXYntXX\nS65EUZq0n5BK6b0dzsCr1Gy0KO37UIbf7E59fby0XwQ81+/X3/PcCfV15GjNd0N44gcEtCDCjR8Q\n0IJocuSejzTQkiYqiSX4qiOa8vGkkc/0T864C46os3KnruOZITql5jhJR5tTQwNSwihmtPkr9DtZ\n48hAEyXIFXdjpi/hSb/Na5MPZCr2dIuZe8WyS9Sw6TNFH+7u792t+jxFLxYLpAtoohzLZVmrjKE+\nu8cJvcTCJxVDwXIEYSymXY4UlaHK0zhriWbbZI3HZfR6bN62PWpXc/LGcsVU1SWXKW4jGWn+JaJZ\nu7u1WzE4IhFv6bZ21ZcfkL6icVXiJOQSY5PdeD4VqhVhpW8y7ROidpVclX2HDqtxy84U3cTSwlmq\nbwR1GjCRCJp7AQEBoyDc+AEBLYhw4wcEtCCaG7LrgUqDKoqZUFlKjkJnmxZuLJHfxsIHTBkBQJrp\nNhOGypoXXB/PhtuCRS+MM5YlHy5fod9Mr5cxSSHHtqxeNwktnrfkbNU3fdbMqP31v/rrqN0/pH1a\nLumXyWh/dOmyi6P2UJ/UWtuwdYcaN2+q1G8bNCHBTFG10/H7S5oCy7Awac4cgxY8QXsXFVsHkGrY\nnTJba/8fJAHMPQeFykqavRdPfnHZiLNkaN+AsxVLXo9LkjBJ3oisZikLsWTqNfC+Uom+Syymj7/8\n7N6o/fgardtfoWtw7ZqXovYnrr1QjXtjq6zBpnX6GNd/8uP1+cTeJx/fOTfLOfeEc269c26dc+7L\njb9PcM6tcM5tavw//kTHCggI+HBgLKZ+BcDveO9PA3ARgN90zp0G4KsAVnrvTwGwsvE6ICDgZwBj\nqZ23D8C+RnvIObcBwAwANwFY3hj2HQBPAvj9dztWPXKv/pExq7lHtJc2tIAKmWVcdmr65Ilq3J4j\nks0UN65EjKicCmWOVU0UWDLG5ZgMd0MZeUku81U2QhmcxmZMwwKF1m3ZokUp1r30As1X5uiMfmA7\nRQa+tPoF1Vcti/ZdhcztuacsVOO+8p9/M2q/uXa96nv25dVRu29IzO1KWZvzJMf3Di16z/4Ilcnu\nbNOU3bzeuVG7e5I+n+WK0JGckWfPiyPxkYRx3Qp0nTHtVy3oq4xLTyWNykWJ1tFqRTq6ljx9z3RC\nf89Vm/ZF7R1vv6T6Fp61LGpfdLHUONh2RN+eEyeKln58kVnvwQP1RtW4rqPgJ9rcc871AjgHwCoA\nUxo/CgCwH8CUUd4WEBDwIcOYb3znXAeAHwL4ivd+kPt8vRzPcUvyOOdud86tds6tLuRyxxsSEBDQ\nZIzpxnfOJVG/6b/rvf/3xp8POOemNfqnATh4vPd67+/w3i/13i/N2JJXAQEBJwUn9PFd3Yn5JoAN\n3vu/pq77ANwK4M8a/997wmMBSDZ8b0uLJMl3bzPCmAUqNV0jf99quacpiypu1HPY4+cQ0pgJHebM\nL6vwU2WFHKLs0kassjIi/qMzYcVHh8RY+oVf/Kzq+8F3vh21E1mhrCr9mkZ7ZpXUUGvv0J/99NNP\nR+15cxdE7dNP1/Xx3t4k+wuTiNoDgDc3bojaKdrzSGZ1WC5L9VfNYpXztAakMX/tFZeqcQ8891zU\n3rBJU1SxuKwBlyeoFE09QoqjtXs7SfJ5kyDVJMN6EVOLilXnKVFNBmeOT6HQCZrv0X79HJwzR6jK\nxed/RPU5Cu8tlOQYzuybLJgsk35Es7MYaCjy5PJ2h+z4GAuPvwzAFwC86Zw7VuHgv6N+w3/fOXcb\ngB0APjOmTwwICDjpGMuu/rOAScQWXPX+TicgIKAZaL7YZiN6KmZMphr9tuRLJoWLSkHHSDijZFKg\nYjHpy5ljJEhsgiPJSiaSrI0s1qLJrEtz+WsWqDCmZ4L2OePGDbjsbInG+tdvf0v1Fah0U4LKPXX0\ndKlxX7jt12S+0BTb333jH6P2oSO7o/a111+jxm2jSL6h9TobMkWRfOecf0HU3rRru54vWaKsLw8A\nnujOqeNEH3/nPi0geQpFK07r6VF9rxHNeOAo6cUbARMWDokZKq7qxfQtUUSeM9dfma6/WtWIoJIe\nvzPXRIxduTgJwaS0W3TksLh4ZSPOkkmKeb9vp2jsp0yJ9ZGpZ0Ttn79isepb/dbe+nte24CxIMTq\nBwS0IMKNHxDQgmiyEIeT3XZjYvs4j9Ko0g46l2Zy5ncrrsww4y7QyxKZcgmjS1+lpItU3ETuUVKQ\ni9NuscnE8cQU1Ap6Hs+tfmPU4yfILWgnvbxzzj1XjauQht0df/Wnqm98pyQBXbzs6qi9aePbaty0\nadOi9uJFOqpvz/498r6du6L2yJCu3op22oE2pchqxNLsPSLJQpm0vuROWyRVX3ft2qn6Dg0Jm1GF\nmL3ZuKaFS3HadTfl1xIkdpKgc1us6vkmUzLOO3NbcAmtgo6MY0erRu5ZMqFN/WRaPptFRQAgO07O\n9aRp86J2uqZ1I198XqoHf/TSU1XflAl18RR7LY6G8MQPCGhBhBs/IKAFEW78gIAWRJOFOGooN7Ki\n0qZsc4zpoJqmMWokaOhI8NGZaLFSWXIBEub4nt6XIB88YSL8KiTk4Cr6+BXaKDhzgfjF67brOmYp\n2qUomurcCVDdPpNIxVFgMRKQKCX1PGZSpN2//psOmPz6n/5F1H752Uej9sqVP1Lj2jNSZ+A3vvzb\nqm86RZnd8CmhH//y7/9Bz5fVTcxeSZIyCufMEmHINpO19uJzL0btwareQ+By1SniDstDR9U4T+tT\nMXXpuO5AkWhWruMAaAEPe1NUSOs+YajEONGHKaLlUu0m+pTKV7/8wmOq75wLpC7gwBHSzt+0UY37\n6BK55jbtOKD6Do3Ur/1CaWyRe+GJHxDQggg3fkBAC6Kppn7MuUigwDltzueLYuapMs0AYmQCZ4kW\nGTRiCgmiYbyJvko4MQEdm3xGX01FBlasqIG83rBlk7zHUChFEuKoFbVJecZpQsMM53TEXI3opsJR\nEcBYtGCBGrfm9bVRe/3adbrv1WejdjtpxfUN9KtxxTS5TMYt2r5dKLxX1gj9mGnTWndVNo8NjVYh\n2vXgIaHz8iUdacja/Oecdbrqmzt5atT+k6/eLvPo0Jr4MxacF7U7O7SJXWa9P6rfVTC0HFdtS2b0\n9yxywtG76FxUC0IRlk2tsAoVFLhwmY6iBLlF8xfLGkyc2auGzR4n9GbG6Of3bdp77GCjT5A/ckyj\nAgIC/kMh3PgBAS2IcOMHBLQgmkzneZQawoisRw4AcfKLvfk9cjXynUjAo91QcSXK0oqbWm5lynRK\nO6bzjN9KdeQ60lrfP0dikzUWqDDzZZ8/3q7nOIUEQnes17XRRvolRJPLWD+zQtM/4yYKnZcwQiI/\nvO/hqN2eldDWz998vRq3+4hQYnd+9xuq7/e++kdRe/1uCd/dtmOXGtc7XXzwdZu0cOil50jNgNWv\nvxm1vREm4XXcc0BTVGtflhDVX/v8z0ft7z38ohq37S15fc0NX1B9u3fK/Kt5oXvTKRPuTeHYNVOT\nwdH5rBkaMEXXYI1KY3uTJZiia6lkM0dpj2VwQOZ46KDOZLzl2huj9soVj6i++b31vaN0eg3GgvDE\nDwhoQYQbPyCgBdHc7DznkGhEVnVUtV5evxPT3ASqKY32GGdYGSGOJJlMlmKrcVYf0YNlo9GeYM19\nU2YpTp/dnpHPKgxpTbwqZXOVjG7/M6tfidpZY6YPbJPstCnTJXtuF5WLtq/bTYbfWwtFoOEv/t8/\njNq79+1W46ZNnSHH271f9T306ONRe+GpQiX2DQ6ocVWiTL3XJvALa16N2heetzRqP/G8NtOzJNyX\nMvRpkVyrx16R7MJEzIhoOKHw9u/bo/qGc3Juenqk2NNzTz2oxp1+9mVRu82UoSrQNZE1F2eR3EtP\nbihfbwDQPUky8Dau1J89Y6bQk+2Tz4napaK+rqZTiTFTsQxJNJTuq5omHw3hiR8Q0IIIN35AQAui\nyUIckizT7/VuepwktDmhBgBSZMqVaFc4bnTeWMssVtO2UJwlu8nUT5ryVzGKGizXjLtAYVtDI3L8\nlJEDT1HykDm8kqu2giPjpkgxojIJlSSM6Tll2uSoPb5dJ6W8/ZZornV1ioZdDVrkYidpuw0amfK5\n88SkHKIwJXezAAAgAElEQVSIyo9etFSNe+JFcVssE5OmSL41a16P2kljphfINN19RNVpQYVM51dI\nwGTavPPUuO3bhTVwpZWq79Jll0Ttx559KGrPmahrvBZywqi4mnY5eEfeVo1JkKtYoN50Wq9HKkmV\nnJ0+yqubhN25cpbckr2989S42//rH8jnOs0M3HLTlY25PouxIDzxAwJaEOHGDwhoQYQbPyCgBdF0\nH/9YFFTSiGiwzHnMCmxQtBSLYcS89n05wsqWvwLtDbA2uo9Zr43mAe3rJSkjypFfXzFCk2US2DTb\nECiQIGPFZHDxmnhajylTdImra66/Lmr/8O7vq75MSjL+tu6QaLpZM+eqcYcP7o3avb29qm/rVill\n9eu/Kllx3/zOv6pxwyT6cO2yZapvG0XyWd+dwWXKjdS9itKc0jNBPrdvuxq3eYtEFB7zdY/hxRef\nj9oTxolf/9oWfYzpk4Q627VP05uzpvVG7bkLdQZhPMV7UySkYi5ALvN94KjeU7ly+cei9kCfrFUs\nra/vYp9kOW4b0tmW1XLpHXN4N5zwie+cyzjnXnLOve6cW+ec+6PG3+c651Y55zY75+52zqVOdKyA\ngIAPB8Zi6hcBXOm9PxvAEgDXOecuAvDnAP7Ge78AQB+A2z64aQYEBLyfGEvtPA/gmG2SbPzzAK4E\n8LnG378D4H8B+Af7foYDkGxEe+VMRBsnTfiS/j2q0ktOSnmHOU+/YxUjxMHVUMH6e0bnPVsWs/2I\n17ZngubMiTnOJGSwi5CKadqyUKFkoYw25ZJJSarpp6q6h4a1Ft2ql1+K2n/wP/+76vvd3/3dqN1J\nlFItr6Pu2smu/o3/8jXVt/R80fG//2FJBsl0aD37roKYrM+vWq36imRycjJVTntF4NWpGRHCuRSp\ntnf9C1G7Z/xkNe6+e0V3cPZkXYZriMri7tktFWzPmjNdjRvMyRpfcuES1ffiixJteMG5varvjTck\nonDqXHlf0ZRVS5OLd/3NukpyhsqNDR2iSMCYvjar5ObecvVZqm/48HYAQK1iQvpGwZg295xz8Ual\n3IMAVgDYAqDf+yimdTeAGaO9PyAg4MOFMd343vuq934JgJkALgCwaKwf4Jy73Tm32jm3Op/LnfgN\nAQEBHzh+IjrPe98P4AkAFwMY51wkcjcTwJ5R3nOH936p935ptq3teEMCAgKajBP6+M65SQDK3vt+\n51wWwNWob+w9AeDTAO4CcCuAe0c/Sh0eQL4RqNpufOsC0W1tKe1bOwpuZZqkavx4T2Ie6ZT5ahQ7\nWyZf0hnRxUEKNc0mNVFRpfBSjrqMpbQfXyvLPCxll/DJ444DgFhCvmdbVsJEU2at9h6UEM+Hn35O\n9TEDeXRA9gkmj9e+799/T3T2f/zjH6u+gZL4u3t275O5t2lhkgqFFXcnjaBJQdaqxFlrGF0EFXmd\njba/JGIh80+T0NsbPvVJNe7QAXnmvLxOC4LcfP2lUfv5N2T+s2dqirQ7K9lzs2fPVH3zZwqV+MpG\nffx7H3k6av+XL30mal97oQ5v/p2/kvLlvZPnqL63tr4VtW/6BfH/rYDpZVeKSOcbb+o9laVn1Wvn\nVcbG5o2Jx58G4DvOuTjqFsL3vff3O+fWA7jLOffHANYA+ObYPjIgIOBkYyy7+m8AOOc4f9+Kur8f\nEBDwM4amR+7FG9lZVRN1x3lPuaIxB0nLjDO22kw5o1qNTWxDxVGJpzRF3TmbWUdv60jq5UmnO6P2\nAJml/QM6EitDNIxJRsNHrrwiar+66gXVt/+QmLZtRMWV8poaSk4g2m+/1mVjDbhxZJp/5Wt/qMb9\n4K67o/YnPnmj6vuXu/8tap+ycH7UftOYuVlyR04740zV98IrkrkXI2pvXFxrIQ4RjRaP6/Neoq8d\noxLi999/vxp30WUSNWivq46E0FvXXiHuwlOvalP5xp/7eNR+9DGtZ1crif90xRn6Gfi3f/LfonbK\niaDGwWF94r/65d+I2mfMald9P3js5ag9eFhcq1ff0ut99rlSzqynW7sq+Vx9sXztfYrcCwgI+I+H\ncOMHBLQgmm7q1xqmSNlogyVo575qEmfSlHzDpnm5pk3DCjEDNSPE4eJiKiZptzSf11Fx1bwkufQn\nO1Rfit4XJ/cjldXm60hOvluX0dXbtFkqoPYN6RJaSTpmtl3M6KOD2pWY0SnVZ9dv1JV6T1ksIRbz\nFp4StSdNmqbGDQ9JJF/fkD5+G7EUa9fK8ZNGKnykIGv8wmot68zMCeuqDJR1ybJ0jM+7ZgYypDGe\naZNzMW9urxo3RLLZnV3ajM7Q+h8qynpPG9+lxk3tleSbmz+pS3RtfV3clgULtDjGpi3bonaRksnm\nztBCH5ff/MtR+7Zf+oTq+9jyq6L2xz9xbdT+8z//uhoXp9Je9/xQC25cfH5dnKRY0kzRaAhP/ICA\nFkS48QMCWhDhxg8IaEG4sSbuvx+YNG26/+QX68IOGVOK+IoLz4/ajz71lOrj8tcgMYysUQBgzf18\nWaeBZVg4gyL+KkbYs4Mi0GxJZ14rRxGECSOb6SmrL21KSxeotHelqKMGM0RpVkhjPpPRoc4j5D9z\n6TEASNC+x7RJUq7r8H5dnupLv/WbUfvv/z+dVHnZctGYf+Flob2GTUYl03ksmgHoCLJUfPRMxkJR\n1rg6pHM5MkTX8p7H8o9crj9rz6qo/dLbO1SfG5Aox9j42VG7zZRfGz9BhE7n9s5SfUsuFgq2/7Au\ne1bOCQWb7pAIv8LAUTWuclj2AqYsOFf15el8fu1rX4nahw7oSMZF5wmdl47pa+fVN+rl0l9a9SoG\nB4esjus7EJ74AQEtiHDjBwS0IJpK5zkHHLPeikVtRu/YKlFKCad/j1IpFnKQ942YMkU1KnmVNMkg\neTK/2YyOx/UxchQuls3oqD6ll0cugnVbykRz9Q1r07aNSjBZEZAy/Q7XaFo5U1qK+zpM5OHwiNCT\nB/aJ8MT0GVPUuL/+q7+L2umsnv+DK56QOdJ80+2a3uQ16GzXCTx9w2KmckRlzDxrUlRPId5hqxOT\nu0bnfdu6tWrcwB5JcpnaM1X1zT99YdQ++3xxYf74f/+xGrerXVyhPdvfVn1dXfK9K+ZcnHHGGVF7\nC1F7uyjxBgAWniG1APpMZOqcuUK7rnlTXBXrrp65TM7TwJ59qi/RmJd7vzT3AgIC/uMh3PgBAS2I\ncOMHBLQgmurjd2TbcMmZZwMAil77ORO7JExy75E+1ZcrCgWWJBqmp1uHXR4akPfFvPZ9vTt+dp71\n8askjlGrWjFMek30VT6v9ysqRD8mDbFSIeqvUtIUGFf2rhBdmDGCIJ7oyIIR+shQuG2SwpRtKexp\nU4Tqq5jf/0JNNNsz3RICW6zoc9aWlHUcyOnQZ6bwKpwxZta7VJD5x00I9gwvfuxTr4rv++u3fUON\n++OvPRy137znedU3s1fqCVz8stTYW71+mxr3K5+V7LyD+/aqvi2bpcT4rr06Y+7oQcmOfGjFY9KR\n0fsVm7bJd7nrzu+pvglThGa8+ZYvRO31b+jszR6qC3B0u57HkeH6OauYNRwN4YkfENCCCDd+QEAL\noqmm/shIDi81yh0XnTZRPUd3wWjR0c8TR8/1DZnSTGRS1mrajE7RV61wseOaqXFF2WLDRR05NbFb\ndOuKHD3ntEuQpWOUjSleJVGHTHp0IZE4lQZPmcjAaoqPr92MWEn6RgpifqfSOoOwf1ii5Gz0YoYE\nSNhMT6WsBiGXLNdrwN97fJt8dv+wzs5ro4IH5Zzuy1DZ8FfffCBq9x3RUXH7DwoVFzdai/9Ooh3/\n7de/GLXnz9VZdt/413ui9pe+oLPnNrz5WtSeu1gLjvzjP98ZtVlH8tqP6lLe//wvUn5s8Zxxqm/8\n6SJk5em+OO10HeF35z+Li7Po1FNU36984ecBAH/6l3dgLAhP/ICAFkS48QMCWhBNTdKZOm2av/XW\neom9AVNGlsUxnNmZLFDGB7sBxsKGo537OMxOOEVBsaBG2evfPnYr4kbTDxRtmEzLrm3JMBQpEhLJ\nZjpV3+HBweOOAwBPpn6edtA7k3oeRZK1zpokoAqJarR3yWcfNkxJinUHk3ohq7Q+SdqFT2Z1slCV\n1rRmoi1jdC44gWdSVgtlHDgsFWAvma5dmj7SGlz9pkTCzZx3hhp3eKvs1nfO0LVe+vtlvTvahQUq\nFTULwZRKoaAj5uZPle/dnjBCF/Q939gpa58wSUCOmJlY1jxvR+R7ejp+OqWZgbdeE8biwgsvVH1P\nPlWXWV+9Zi2GhkZCkk5AQMA7EW78gIAWRLjxAwJaEE2l82rOYbjhryZNZl2CqKGYqX/NPmixKv5X\nW1z78ZydZ8tO1cifznO2WFz72SzmUavpTCxHdFapJPNIms+K07Ie7T+i+iZRtOGRAU1HclRflgRH\nNMkFxOJyfFtGLEH1CXkLIWWy+NKUTVcw+zxtaVoDoiZN0B3KNF9vCgiUKQOtRHs0QzFNP8aLso7f\nukeXA/u1W6Rk1OypUhp717bX1bhde8W3PmuyPmeTJ0m23vAIrbcp+eWJFk2a0lUbd4j4xoZ1L6u+\ns84XoUxH571c1t+zRmsQNyc0QXUYDm/fFbUXnH26GreYdPWnzFio+pYt2QAAeGvj2G7pMT/xG6Wy\n1zjn7m+8nuucW+Wc2+ycu9s5lzrRMQICAj4c+ElM/S8D2ECv/xzA33jvFwDoA3Db+zmxgICADw5j\nsgucczMB/ByAPwHwX12dN7sSwOcaQ74D4H8B+IfjHqCBbDqDM0+pl2R6bYPWg2dNfGdMylPnzYja\nm3dIwoQ1c4sUrVfz+iAVcgM4Sadc1sdIt8mSWKqzQrrp3ZS8cnRQR/hVSA/Nx/US7+sTbfe0oQvT\nngUrxNzuMK6Eqj5rSkZ1dompP9gnyTbJtKE32YQ3kZLMaaYSEnWXL2pNPBYqceYZUqGTmCzIGi87\nX1eRffyhR6P2dddeofrWbhKzt3eqmOY/eHijGnf1FfI+ux65gpwbdqUSxp3MktjG8Ig20zPtMnbp\nxdeoPqYBY3RdFQr62onTuGrCJH8RhTdYFdcwX9KUI9eA2EwUJgC0jz8LAODiur7BaBjrE/9vAfwe\nEF0hPQD6vY/upt0AZhzvjQEBAR8+nPDGd859HMBB7/0rJxo7yvtvd86tds6tHrax9QEBAScFYzH1\nlwG40Tn3MQAZAF0Avg5gnHMu0XjqzwSw53hv9t7fAeAOAJgzd17zwgQDAgJGxQlvfO/91wB8DQCc\nc8sB/K73/vPOuR8A+DSAuwDcCuDeEx0rXyjgjYZvf86iBapvLdWAc8bJHxkU35Iz2NLGBypRDTtL\nyZRK4ldVK9LnjS49a+5bXf3OlPjPfaQBb7PWClV5X8zoIkztETpvwNTEGzdOdNmPDEoGWsyKitIa\n2O85kqOsO+LfLEWaYIo0r0NUk6Qekqdy4DUThlok4clOQ48VqK+jQ/znl17TVByv/pLFmqJa84qE\nGb++TdZq0rgeNa4MOe8pE36cywt31klinlVTY25wRNbARrGnSFSjbERiq2Uq800ZlXDm1qJpJc2W\nSomuxxnTJOR4uE/vHc2eK5Tm0SO6TsKxfSt774yG9xLA8/uob/RtRt3n/+Z7OFZAQEAT8RMF8Hjv\nnwTwZKO9FcAF7zY+ICDgw4mmRu55iGm3eIE263pnzoza963UJbQODkpJ5zhl9eVNyFCaoqOqSW3M\npMikL5GL0GY05Tkib0K7Lpc8lBdz0xNtFDe0oorSMiIa/cNkGhrze3BQ6LcFk0SEYv+AzqyrkenP\nFCMAxBOsYUca/lb8j96WdppWrJIWILOicVPavErHPzii55ikS8snZH3YbQMAUAThPY++qrouXXpa\n1N5+RCLmTj//IjUuHqOISrPe6TY5foWo1WSHvniKI+TSjOhN6KGDovdXMGXPDh6R7zN1POk6TtTX\nd4JKgHtDNXsSO+HvUjM065HDcv0N9mv3bP2qu+tzNdfKaAix+gEBLYhw4wcEtCCaaupP6O7GL1x/\nLQDgCEWVAcDGrVK2yFiv8KRvFyORDpfUOnIl2k3/zFVXqb6tVKJr4y6JCBvOaZOJSzwVjJ4dC2WA\ndOliGW02OpKMRml0BrNsBEccKYvsOizlr9IJ7Y7UYjKO9fEAXY22TElGzgiOOFa8zug+jojkOZWM\nNl+Ctr/bs7q81vTJ4qqwXHVHmym1RcIhiXZ9Ph97TqLQuntEgjpn1i2RkfdlM3o98gVxTxJKbEO7\nHL5MJnxczyM9Yb50lXWGTWdJIkmLtKibXntajTvvguuidsW4THQ64YiJsVWYueycTS7rmVZP6Ems\nP4SxIDzxAwJaEOHGDwhoQYQbPyCgBdFcOq/mUSzW/ZvJE3X01cCw+ITLL1mm+r73ox/JMVgow5QR\n/vR1V0ftduNLzpg+PWq/uWlr1O60mW8sEmlKXCVI9JJ1Jyql0em8lIms4/fFYuazSYu+SrWwC6bk\nN4uR5nJGLIRcP87Iy5iS30MU0Rar6DmmSZu+SHsG6biJhqTMxo609jkLRMFecemlUfuhRx9T42JE\nWVVNqbD2KbPkBfm+GVMeLUZfulAyezYqEpNEWw2lxpGNzoibFnIkbtqp9zKmzhN9e6ZBJ0/Xuvec\nfZox+yHliuw3lCkDz6rJbnpdSmr19es9ihs/fgMAIJ3W+0GjITzxAwJaEOHGDwhoQTRVV3/y1On+\n07feDgD4pU/rMkXFETFdYkZzv3ucmEb/cNddUftTH71ajZs4QaqJmqA4rF63Lmq/sl6ow5gpcZWk\nEkY+Y3TZqPxVgiID2RwGgA5KWBm0Zbio4mkur5OMuijJ6MgI9VW0y8FnrAoTuQdZuyrRRl2dWs+e\ny4gNjehkIa64285lvoyu3gi5C2mrf0gJTikyP/uH9Gdxqa2aCS7sItOfNRTjRsO/f2AIo6FMVDDr\nDtZKmpZj3buMEUgZoojKmIly9ERxKmEYs1YxouaqhhYFRYFWqpwspMft3y+JOWlDwVYaIinPPPsC\n+vsHgq5+QEDAOxFu/ICAFkS48QMCWhBNpfPGj+vGz//c9QCAgYEB1ffw089E7U9df53qY9/p0nPP\nidqTeiaocWnyCTOGzjvndKm3tmat+PuplBGQIL84bUJDY+TX58rify2/4Hw1btVrkklmy0cPkD+d\nNL7qcEH84s5OqXuXrmrK7giJezpD9bGIhiefNm/qwaUzMq9sQoeoFisylmmuuBWoIN80ZcpTTxwn\npaA37JGQ3QkdupZgmXT1+fsDQI5oUj6fR48eVuNiRJVZ9zlJe1ieREpThrKrkXBIf0Wv6bgu2ZcZ\nHNZ7FExxVugYCVMXkTMZ02atKkWZdIJEXXI5vQc0fZJcq10d+vo+cmjwuJ87GsITPyCgBRFu/ICA\nFkRzI/e8j0omP71Ki/befN21UTtrNOAPUinlRXPnRe1MRpuoBYrky/dpQYIYUTS3f/7zUbu/T7sc\nD6yUyLJSTZtNLMHfkSY6L6ePUSLTzc6xSBl/+YqmlKb3SLmnA1zu2pAzVaofYKnPQlH6spQ12N2t\nTezCkLgLiZq2j9vahfrLpMScz41oWrFck+/S36f7jtK6Zkirr2g064pEVXZ2acoxR/qBuZysVcLU\nQmB3pGrEK3jpuFR1zTzzuFRYyunjD/bLd6kY+tSD1pvKcFdMBKEnF7Jq3D+m/uIJqkdQPaqGbdss\n9WwuX3au6ssnJzQO9T6X0AoICPiPg3DjBwS0IJobuTdtuv/UrV8C8M7IOg7b+sKnb1Jd/VRVtqdb\ndPCslPCUKZPkxbt8rwMHReSip0cnC/HO7N9/507Vx1VxWXrbzkMlfJi+NorqK5vdek42KZG2Wxl6\nXCYjMt8pk8hRJleCz23/iHYr2A2YOE67AQMUCddOZv/QsBGvIKGPiZ3jVF+RdA2P0k54ykT4gZkT\nZ8x00p/ztFsdy2u3gpmTkonIK5L758Alv/Saxuk82crCJZpj0URbJuiccdk2e95rR4TZ8JOmq763\n3ngtal91bm/UPjKsRTX+/dE3ovYtN1ys+rKN++Ib37kPe/cdDpF7AQEB70S48QMCWhDhxg8IaEE0\nlc4b19mNm6+qR+WljHDDnQ+tiNr9fTrbKkFlhfsHpS/bpqmyJNFGIyM6K66rS6iWJJVEOnhQR4Gt\nfGFV1P7k1R9RfQ89JXr/nIHHJa0BYOGpvVH7TcoEBDSddfVlWnDkmdUS8Veg6LHrL7pEjXv0RRlX\n85oeS5F2/OxZIlC5detWNW7ZeVKuemDQ0EYUlZgj2i9jaKhBYqwOD2nx1Jk9E+UFCX3mCtpHZoHK\nlHa74ajWVKXAAqB63EhR/Pq409dVMk3lr2j/o2r2RpJ0PgumNFucfPdazWRK0jUXJ7GQakXPo4/2\nK1598Eeqr4MyNt/YIhr+beO1YMf1V0rU6o7d2v+v7a/T1zZCczSM6cZ3zm0HMASgCqDivV/qnJsA\n4G4AvQC2A/iM935sav4BAQEnFT+Jqf8R7/0S7/2xR8VXAaz03p8CYGXjdUBAwM8AxkTnNZ74S733\nh+lvGwEs997vc85NA/Ck9/7UdzvOlGnT/S/8p18FABg5O3iIifLZ6z+u+n60cmXU/tS1H43abUZc\nYtJ4oZScMfk6O4UGHBwSd2HfPl11lCu7fv/++1Xf4lOlkunLr0vVVxvhlyCT7+rLrlR9z7woeuvx\nirZtC3wuiN6cO2eWGrdlx/aofeUFupxUN63Buk2b5D3bt6hxSUpSyRl6rINoS74+akY/8LJzpXTi\n488+r/qYpnMUTTdihE/GtQk16Qy9WaYIvTzr3pvovCxF5OWKmnJMUCKUCVDU0y0QDWhENED0XrGk\nk3TOIHfqzrv/KWpffNGZatzuw3KdxdwU1TdlvFzHW7ZL1ejD/dp9WrBQyszNnD5Z9W14vX5+n3nx\nTfQPDL9vdJ4H8Khz7hXn3O3H5uu939do7wcw5fhvDQgI+LBhrJt7l3rv9zjnJgNY4Zx7izu9997Z\nR2wDjR+K2wGgs6v7eEMCAgKajDE98b33exr/HwRwD+rlsQ80THw0/j84ynvv8N4v9d4vzZJZFxAQ\ncPJwwie+c64dQMx7P9RoXwPgfwO4D8CtAP6s8f+9JzpWd1cnPnbNFQCAHz2qa4uVC2Iw3P/ECtX3\nuU/eGLWrRFfkRjTt4khss2i07jvJLyxTOGxbRgsacMhu1dQ4W7VGarmxMGR7RlNDVaJ/Vjyuv8v8\n6RJW3G9CYDNc944ozDNP0SWXF/bOidoxs7/wwIpHovYNV4ugyVubNK145gLZjtm4Tfv/nOFXIWWL\nTEWHsj733LNR265VnjZxJrTLvkNXXNNNeRLiqJmMtlRWPi9FpcFrhj4tUD27pLmkubZgf54yEqER\no/Vm6hcAjgyIr10c1iHB37rr7qi9YO7cqP3//OU31LgR2ldqy2oXPNMm18Rvf0X2sP7gc3+nxiXi\ncq3u26nPZ6JaX6vVr2/CWDAWU38KgHsasccJAN/z3j/snHsZwPedc7cB2AHgM2P6xICAgJOOE974\n3vutAM4+zt+PALjqne8ICAj4sKP52Xn/qZ6dV6uOntGWNKWaLjlL9PLaicLr7tSbhXyMaYbuSHFp\naRLH6OsfVOPKpMt2dFD3rVnzatTeeVDEQcp2CSnLrPYuHFIypk3W665aLnMkd8GKVzz2vJjY1yzX\ndOHbG2Xf9bQzFkft7Vt3qHFTpwsJ8+hK7XZx6S1P1FbZuE/V8ugZiu1UkyBH5nzM7AGzW+FMffQO\ncsOYzqvVNO2XJPq0YnTqSiScwec9bmoVzF0oVO3mzZtVX4lckA1vrlZ9558lJbTPnynrNue8G9S4\n19cK/Xv9tdeqvq6Z4nZ1tMn1bd2nEXJt7TVxcE/9/H72F2/FuvUbQnZeQEDAOxFu/ICAFkS48QMC\nWhBNFtsEyo2spbgRQK+S7+dNWOczr78ZtW+4WvYTvQndLBMV13dY5wv1Utgr7wVM7BmvxrGvms/r\nDL8tByUjqisrvljZjMsShZTLa19s5hRR/CmaMt/PvyBlkE+d1xu1J4yfqMZdf8UVUTue0oo2ixYJ\n9VfOyfGTSb2f0JYU/3nmjGmqby+FMVdIrjJhZJM8fU9nCt8NkFIN72XYvQBfPH6NPQAYpky+BGXB\nJU2WIAtl1tKackzRnJniTZq9rXUbZG8kZvYaOCPv1EWnqb6uLpnz1oJcE0t69B7T9R8XSrpjglbg\nKebk+klSdqWqxQcdZm2vzWlz6qHDSXM9jIbwxA8IaEGEGz8goAXRVFMfToQRraBBmszBiqHAJlC5\noBUkhjFzkhbKPGvx6VE7ZsxS1mhPUJYZZ+MBwOEj4iJYU6unWyK6WDf+3NMWqXE79+yO2jVT4qqP\novW623UI86xJEuF2Sq/UD7Dm8etrxfWZP3++6nv6iSejdqZD5rtg7kw17sEnZZxhT3E9uVOb3hYT\n+MAR7T6N5OW75Qw9lqaMtgKVF29L6EuuKyumcsnr79lDtQC2bJHowglTp6pxHfRZ3rgBhYJ8djxF\n5cWq+kvz9Vco6Oi8rpTQgNXyEdU3kBNX8fRZMt9ETWfxpbpk/Udy+viO6hPU2uUYLHQK6OvApsYc\nPVKfV602Nno+PPEDAloQ4cYPCGhBNL+EViOKK2nKZBWrsgOdSOi+I2RWJ9Ji7hzer7Xi3nYctXaG\n6psymXbGSZzBRi5ydNezL2hxiSKZjZ7M0pnG9Dx1wYKoff+jj6g+T+bamaedrvpyfZIMUqXd9COH\ntXk5i3b8WYQCAC66SIQ5urrFjVn3lo5Gq3IpKKN1d99DD8jxM+IuDOR0JGOaElu6szrZqUCRdvNm\nSBLKvr1a47BEJnfBuAtlyHmfPUdcmlxZR+cNU7SedRN7qGbAINULyJkoxHScKtYatmg4L+5ZV4dm\ngQZHxEyf1CGRpDmTcDSBzlOHcfG4LkB/nzBHw4Nae9KRTzZp0iTVlxuqR+7VTIXn0RCe+AEBLYhw\n47dJPdgAAA1eSURBVAcEtCDCjR8Q0IJoqo/v4KIIrJoRmqwQ7eUq2j/KUkRXnjKW4gk9btuB/VH7\nlMU6wmpkWPzAeFJ+7+KG9qvSvC41Qpnf/7HooXd1ip/26PPP6M/KiY9oAslUDbjnV72k+nrGT4ja\nG3dvj9pTJmk5wyMj4vudPnue6jt8RKLuHn9ZMsmuuWy5GvfaRim5nExpSvPUhbJ22/fsitoT2/W4\nQfKtB01tPl7W7XtkTumkroXAeyWcZQcAtcrx6+pZmrVC/noqq/c88gW5XlJ0MlKGVuS9nqrXJ22I\nronDOmAOv3S91Cd48AURwXjg6RfVuD/9s/Oj9sDh/aqvnepBDh6QLMrJvfoa3r99e9TOxPXe1LRG\nZGoqRO4FBASMhnDjBwS0IJobuQcf0Q1xE402LiNRSgtm6iizDTt2Ru2kpwgro4DR3i4m2pPPPqf6\nbvo5ET9IFeUY72YazZmuk2PS5HKUSBRhRu8MNW7rTimJnMnoxJNf+flPRO1/uvse1TdMyRqsx3do\nQFNxv/hzIvJQrukkII7uemuHRBBaUYeFkyWJZEu/1v7bsEk+j5NjENM0F5vHXgfMIU3vy1FJLhY6\nAYB0Sp49lbJ+DtXo83I5iYRLGAqzHBc3w9KzLkFCIqTr6M26cYBlytDJadbtH9IU8j1PiGlepkSl\nQ2ZNr7zqsqh941Vac3/yVHk9eYbo9sU2r1fjbrzhi1H79dVPqb71G7cDAPqOjq2YVXjiBwS0IMKN\nHxDQggg3fkBAC6LJPr6LSg7XbEqYk6ms3a6FIbn0sSMfLm7EKnMk4HET1dgDgLt/IFRciYQbPvOx\n69S4+b0i2JFIaFpnyjjJBlx2/nlR+wcPP6DGJen4hbwOL/36t0WHvSOjj88a9hNJ233xIq2r/8Zb\noql+2vw5qm/3AdlfqJJv/dAT2idk6qyU0/5utk3m1dkmobhDeU2fxolSyib0uSgRFZcmGi1X0J+V\n8CSwYcK4y+QnV+lcWP39bErmGIPeOxqhcNsEvS+Z0LRiukuOMTJsqUmZY7u55iaMFyqub0D8697Z\nc9W4yZNEfKNtsj5n67bJ9b5ji/j1Tz2/So179gkR7Fxyzvmq797G9d3fp+vtjYbwxA8IaEGEGz8g\noAXRZFMfqB37qTGCAYOkYd9utNcS9Jp12GLQx5g3QyLcciaz6eprlstnDYn598OHHlbjfvtLvxy1\nyya6kLX0hrmUlykznU2LGZlI6t/WT1wutM4DT+uIv+Kg0HmeIgO379SuT3uXRPjd87g+BtckYOrM\naFygh6PwzLmoKlEKKnFltOhYJ7GW0/RVnNagyLr6JlKyRtl5FVM7vb1dzOiRIpnfJrKzlhCXxpnj\np+Jy7XBmZMkcg6tfp4yL1z8i5vPwkb2q73O/8mtR+/t33RW1i6bkdzIp83j2cZ2x2TNNSm0vXnpx\n1J62d6ca9+CKB6P2a2++qvpSyfq5qHmTajkKxvTEd86Nc879m3PuLefcBufcxc65Cc65Fc65TY3/\nx5/4SAEBAR8GjNXU/zqAh733i1Avp7UBwFcBrPTenwJgZeN1QEDAzwBOWELLOdcN4DUA8zwNds5t\nBLDce7+vUSb7Se/9qaMdBwAmTZ3uP3nrrwIAEsb0LFFkWbZNm/pVSsLgyltGdkxrknkjtEDJIb0z\nRU76wFEtDPHxj4je3LgurXnGJYyefFF2XHfs26fGfe4TN0XtB1boarlFqojrnd4hHs00z2bMbje5\nIDMnTlB9u/eLaEeGFnlcmxbK2Lpbkm+SRrsh3knVYomhSJnnRIVM/5qJDJw5VaIed+0TcQkWOgGA\nPLEeXW1WXpvKaxHjUTMuHuvnOWcTeMhVofmnrdw4sRL7D+ud8e5O2vEf0lp6mQ65RjIpcRFWv/Ck\nGsel1GyZrzPOksi98VPl2ty2c4MaF/OydpcvmaX6nn7hDQDAI0+9jKP9g+9LCa25AA4B+JZzbo1z\n7huNctlTvPfHrvj9qFfVDQgI+BnAWG78BIBzAfyD9/4cACMwZn3DEjiu6eCcu905t9o5t7qQzx1v\nSEBAQJMxlht/N4Dd3vtjtu2/of5DcKBh4qPx/8Hjvdl7f4f3fqn3fmkm23a8IQEBAU3GCek87/1+\n59wu59yp3vuNAK4CsL7x71YAf9b4/94xHAulUt2/KTv9mxMnn6s9o/3RvqJQc21EE42YUsGg/Yqb\nPqJFNF5ZsyZqJyhqbaBf034PPCv02C3X6ai+8ePE973u8suj9soXdenkDopUu+XmT5opUuSh0YAv\nU1Tbd+/98XH/DgBZKt/Vf3RA9bVnZX24vPNeU1KMqbL507Rw49rtktXHtQRKJhuyRGWcJo4fp/r4\n89ivt5r1IGESGxlYIeHIbjrvlj6tVSg7L6bdW6ZW80WyOI3YZrZbojLby5oSKxOVmDVCmUwLZmkf\nacHiJWrcIw/JXs8NN16j+l5ZLaXTph6W/ZAtezSdN2eqzLGtfYHqO5qvr2N19KrsCmPl8X8LwHed\ncykAWwH8MurWwvedc7cB2AHgM2M8VkBAwEnGmG587/1rAJYep+uq4/wtICDgQ46mRu7FYg4dDTO+\nbGhE1lE7ZBINxrWJeTU8LKb51KnaRB0YFKrl2dXa/M5RVNjAbjFlp07UZMTgkJiojxkxj8suvDBq\n33mPJP381i9/0XyWmH9Dw1qL/uAR+W4LZmvBkUkTxVz+5V8QA+pb371TjWuns5bKdqo+No9LZFan\ns5qaBCWzXLz0PNWzbb+sAbOicbN/61IykYER7TIxfVooiImdSRmqliLNysaVSFEEYblMlK4pS1ah\neXWkNF2YIYqw6OSzYtqrwOF+OU9tZo5HhqTPJgh1jpe4tUJFXLJ7H/iBHkcuxwP368i9G28Ud7AM\nOUapqN2zTIeIpxzcdUD1bdtZp2eLZn6jIcTqBwS0IMKNHxDQggg3fkBAC6KpPn6t5jFUOL4Pkibx\nB5stVqT9gPMXidb4y5s3qXEJ8lvz0JRPLMa67PJZ7RO71bj+YaHH3t6u6ZTJneKDlylc9a+/+X/U\nuBkUdtlvwj9v/JiIfsaSevnLJCQygerewWSL5SvyPQfNHkKOKM42ymqMmcy6IlFUjz1r6EiiBAcp\nTNkbrqiLhESGhjXlWIvLea4SPZavarqth/Y1+vv1d+G45RpRcZPatQ8+WBCfP96mtf9HhmXfp1qU\na8LWr+sflOMzJQoAccheQ8c4TVvmh4XSnD5d9myuXq6FYIZpHsNDej9k9RMPRe09I7IGV15zhRpX\nHJD1/8a9Wrd/+bL63vs9ps7iaAhP/ICAFkS48QMCWhAnzM57Xz/MuUOoB/tMBHD4BMM/aHwY5gCE\neViEeWj8pPOY472fdKJBTb3xow91brX3/ngBQS01hzCPMI+TNY9g6gcEtCDCjR8Q0II4WTf+HSfp\ncxkfhjkAYR4WYR4aH8g8ToqPHxAQcHIRTP2AgBZEU29859x1zrmNzrnNzrmmqfI65/7JOXfQObeW\n/tZ0eXDn3Czn3BPOufXOuXXOuS+fjLk45zLOuZecc6835vFHjb/Pdc6tapyfuxv6Cx84nHPxhp7j\n/SdrHs657c65N51zrznnVjf+djKukaZI2TftxnfOxQH8PYDrAZwG4Bbn3Gnv/q73Dd8GcJ3528mQ\nB68A+B3v/WkALgLwm401aPZcigCu9N6fDWAJgOuccxcB+HMAf+O9XwCgD8BtH/A8juHLqEu2H8PJ\nmsdHvPdLiD47GddIc6TsvfdN+QfgYgCP0OuvAfhaEz+/F8Baer0RwLRGexqAjc2aC83hXgBXn8y5\nAGgD8CqAC1EPFEkc73x9gJ8/s3ExXwngftSFAk7GPLYDmGj+1tTzAqAbwDY09t4+yHk009SfAWAX\nvd7d+NvJwkmVB3fO9QI4B8CqkzGXhnn9GuoiqSsAbAHQ732kjNGs8/O3AH4PwLEMlJ6TNA8P4FHn\n3CvOudsbf2v2eWmalH3Y3MO7y4N/EHDOdQD4IYCveO9VSlqz5uK9r3rvl6D+xL0AwKIP+jMtnHMf\nB3DQe/9Ksz/7OLjUe38u6q7obzrnLufOJp2X9yRl/5OgmTf+HgBc/mNm428nC2OSB3+/4ZxLon7T\nf9d7/+8ncy4A4L3vB/AE6ib1OOfcsVzhZpyfZQBudM5tB3AX6ub+10/CPOC939P4/yCAe1D/MWz2\neXlPUvY/CZp5478M4JTGjm0KwGcB3NfEz7e4D3VZcGCM8uDvFa5e4+ubADZ47//6ZM3FOTfJOTeu\n0c6ivs+wAfUfgE83ax7e+69572d673tRvx4e995/vtnzcM61O+c6j7UBXANgLZp8Xrz3+wHscs4d\nK0V3TMr+/Z/HB71pYjYpPgbgbdT9yf/RxM+9E8A+AGXUf1VvQ92XXAlgE4DHAExowjwuRd1MewP1\neoSvNdakqXMBcBaANY15rAXwh42/zwPwEoDNAH4AIN3Ec7QcwP0nYx6Nz3u98W/dsWvzJF0jSwCs\nbpybHwEY/0HMI0TuBQS0IMLmXkBACyLc+AEBLYhw4wcEtCDCjR8Q0IIIN35AQAsi3PgBAS2IcOMH\nBLQgwo0fENCC+L8sNFksNV8t2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2504696a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[99])\n",
    "scaler.inverse_transform(Y)[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keras inplementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = list(X.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = k.layers.Input(shape=img_shape)\n",
    "\n",
    "net = inputs\n",
    "net = k.layers.Conv2D(9,3,activation=None)(net)\n",
    "net = k.layers.MaxPool2D()(net)\n",
    "net = k.layers.BatchNormalization()(net)\n",
    "net = k.layers.Activation(\"relu\")(net)\n",
    "\n",
    "net = k.layers.Conv2D(3,2,activation=None)(net)\n",
    "net = k.layers.MaxPool2D()(net)\n",
    "net = k.layers.BatchNormalization()(net)\n",
    "net = k.layers.Activation(\"relu\")(net)\n",
    "\n",
    "net = k.layers.Flatten()(net)\n",
    "\n",
    "net = k.layers.Dense(500,activation=\"relu\")(net)\n",
    "net = k.layers.Dense(150,activation=\"relu\")(net)\n",
    "net = k.layers.Dropout(0.6)(net)\n",
    "\n",
    "net = k.layers.Dense(5,activation=\"linear\")(net)\n",
    "\n",
    "outputs = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = k.models.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=k.optimizers.Adam(),loss=k.losses.mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "70/70 [==============================] - 0s - loss: 1.1004 - val_loss: 0.6405\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 0s - loss: 0.7770 - val_loss: 0.6300\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 0s - loss: 0.6295 - val_loss: 0.6299\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 0s - loss: 0.5234 - val_loss: 0.6232\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 0s - loss: 0.4710 - val_loss: 0.6245\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 0s - loss: 0.4657 - val_loss: 0.6100\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 0s - loss: 0.4298 - val_loss: 0.6008\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - 0s - loss: 0.4506 - val_loss: 0.5992\n",
      "Epoch 9/10\n",
      "70/70 [==============================] - 0s - loss: 0.4312 - val_loss: 0.6036\n",
      "Epoch 10/10\n",
      "70/70 [==============================] - 0s - loss: 0.3991 - val_loss: 0.5943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.keras.python.keras.callbacks.History at 0x7f24b67bbf60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y, epochs=10, batch_size=5,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.,   4.,  35.,  19.,  13.],\n",
       "       [  4.,   3.,  25.,  13.,   9.],\n",
       "       [  4.,   3.,  26.,  13.,  11.],\n",
       "       [  6.,   5.,  43.,  19.,  18.],\n",
       "       [  5.,   4.,  36.,  17.,  13.],\n",
       "       [  5.,   4.,  34.,  17.,  13.],\n",
       "       [  3.,   3.,  27.,  14.,   8.],\n",
       "       [  5.,   4.,  34.,  17.,  14.],\n",
       "       [  4.,   3.,  32.,  16.,  10.],\n",
       "       [  6.,   3.,  40.,  17.,  19.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(scaler.inverse_transform(model.predict(X[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    4.,    0.,    0.,    0.],\n",
       "       [   7.,    4.,   10.,    1.,    0.],\n",
       "       [   1.,    0.,    0.,    0.,    0.],\n",
       "       [  13.,   16.,   48.,    3.,   33.],\n",
       "       [   3.,    2.,   25.,   15.,    0.],\n",
       "       [   4.,    7.,  100.,   27.,    0.],\n",
       "       [   0.,    0.,    4.,   15.,    0.],\n",
       "       [  15.,    0.,   85.,   18.,   59.],\n",
       "       [   5.,   10.,   66.,   24.,    0.],\n",
       "       [  28.,    4.,  338.,   47.,  189.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(scaler.inverse_transform(Y[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70 samples, validate on 30 samples\n",
      "Epoch 1/50\n",
      "70/70 [==============================] - 0s - loss: 0.4058 - val_loss: 0.5917\n",
      "Epoch 2/50\n",
      "70/70 [==============================] - 0s - loss: 0.4057 - val_loss: 0.5838\n",
      "Epoch 3/50\n",
      "70/70 [==============================] - 0s - loss: 0.4148 - val_loss: 0.5721\n",
      "Epoch 4/50\n",
      "70/70 [==============================] - 0s - loss: 0.3570 - val_loss: 0.5430\n",
      "Epoch 5/50\n",
      "70/70 [==============================] - 0s - loss: 0.3739 - val_loss: 0.5396\n",
      "Epoch 6/50\n",
      "70/70 [==============================] - 0s - loss: 0.3704 - val_loss: 0.5497\n",
      "Epoch 7/50\n",
      "70/70 [==============================] - 0s - loss: 0.3981 - val_loss: 0.5263\n",
      "Epoch 8/50\n",
      "70/70 [==============================] - 0s - loss: 0.3629 - val_loss: 0.4919\n",
      "Epoch 9/50\n",
      "70/70 [==============================] - 0s - loss: 0.3841 - val_loss: 0.5291\n",
      "Epoch 10/50\n",
      "70/70 [==============================] - 0s - loss: 0.3511 - val_loss: 0.4915\n",
      "Epoch 11/50\n",
      "70/70 [==============================] - 0s - loss: 0.3571 - val_loss: 0.4636\n",
      "Epoch 12/50\n",
      "70/70 [==============================] - 0s - loss: 0.3062 - val_loss: 0.4328\n",
      "Epoch 13/50\n",
      "70/70 [==============================] - 0s - loss: 0.3104 - val_loss: 0.4433\n",
      "Epoch 14/50\n",
      "70/70 [==============================] - 0s - loss: 0.3635 - val_loss: 0.4276\n",
      "Epoch 15/50\n",
      "70/70 [==============================] - 0s - loss: 0.3267 - val_loss: 0.4457\n",
      "Epoch 16/50\n",
      "70/70 [==============================] - 0s - loss: 0.3280 - val_loss: 0.4083\n",
      "Epoch 17/50\n",
      "70/70 [==============================] - 0s - loss: 0.3354 - val_loss: 0.3878\n",
      "Epoch 18/50\n",
      "70/70 [==============================] - 0s - loss: 0.3392 - val_loss: 0.3919\n",
      "Epoch 19/50\n",
      "70/70 [==============================] - 0s - loss: 0.3555 - val_loss: 0.3728\n",
      "Epoch 20/50\n",
      "70/70 [==============================] - 0s - loss: 0.2863 - val_loss: 0.3271\n",
      "Epoch 21/50\n",
      "70/70 [==============================] - 0s - loss: 0.3118 - val_loss: 0.3635\n",
      "Epoch 22/50\n",
      "70/70 [==============================] - 0s - loss: 0.3261 - val_loss: 0.3376\n",
      "Epoch 23/50\n",
      "70/70 [==============================] - 0s - loss: 0.2859 - val_loss: 0.2740\n",
      "Epoch 24/50\n",
      "70/70 [==============================] - 0s - loss: 0.3271 - val_loss: 0.3421\n",
      "Epoch 25/50\n",
      "70/70 [==============================] - 0s - loss: 0.3020 - val_loss: 0.3414\n",
      "Epoch 26/50\n",
      "70/70 [==============================] - 0s - loss: 0.3388 - val_loss: 0.2892\n",
      "Epoch 27/50\n",
      "70/70 [==============================] - 0s - loss: 0.3263 - val_loss: 0.3313\n",
      "Epoch 28/50\n",
      "70/70 [==============================] - 0s - loss: 0.3327 - val_loss: 0.4062\n",
      "Epoch 29/50\n",
      "70/70 [==============================] - 0s - loss: 0.3458 - val_loss: 0.3374\n",
      "Epoch 30/50\n",
      "70/70 [==============================] - 0s - loss: 0.2890 - val_loss: 0.3115\n",
      "Epoch 31/50\n",
      "70/70 [==============================] - 0s - loss: 0.2830 - val_loss: 0.4141\n",
      "Epoch 32/50\n",
      "70/70 [==============================] - 0s - loss: 0.3643 - val_loss: 0.3660\n",
      "Epoch 33/50\n",
      "70/70 [==============================] - 0s - loss: 0.3058 - val_loss: 0.3806\n",
      "Epoch 34/50\n",
      "70/70 [==============================] - 0s - loss: 0.2669 - val_loss: 0.3325\n",
      "Epoch 35/50\n",
      "70/70 [==============================] - 0s - loss: 0.3430 - val_loss: 0.3681\n",
      "Epoch 36/50\n",
      "70/70 [==============================] - 0s - loss: 0.2988 - val_loss: 0.2972\n",
      "Epoch 37/50\n",
      "70/70 [==============================] - 0s - loss: 0.2834 - val_loss: 0.2424\n",
      "Epoch 38/50\n",
      "70/70 [==============================] - 0s - loss: 0.2818 - val_loss: 0.2621ss: 0.2\n",
      "Epoch 39/50\n",
      "70/70 [==============================] - 0s - loss: 0.2933 - val_loss: 0.2702\n",
      "Epoch 40/50\n",
      "70/70 [==============================] - 0s - loss: 0.3204 - val_loss: 0.2889\n",
      "Epoch 41/50\n",
      "70/70 [==============================] - 0s - loss: 0.2804 - val_loss: 0.3097\n",
      "Epoch 42/50\n",
      "70/70 [==============================] - 0s - loss: 0.2951 - val_loss: 0.2441\n",
      "Epoch 43/50\n",
      "70/70 [==============================] - 0s - loss: 0.2803 - val_loss: 0.2647\n",
      "Epoch 44/50\n",
      "70/70 [==============================] - 0s - loss: 0.2682 - val_loss: 0.2713\n",
      "Epoch 45/50\n",
      "70/70 [==============================] - 0s - loss: 0.2891 - val_loss: 0.2681\n",
      "Epoch 46/50\n",
      "70/70 [==============================] - 0s - loss: 0.3070 - val_loss: 0.2541\n",
      "Epoch 47/50\n",
      "70/70 [==============================] - 0s - loss: 0.2638 - val_loss: 0.1979\n",
      "Epoch 48/50\n",
      "70/70 [==============================] - 0s - loss: 0.2463 - val_loss: 0.1928\n",
      "Epoch 49/50\n",
      "70/70 [==============================] - 0s - loss: 0.2692 - val_loss: 0.1953\n",
      "Epoch 50/50\n",
      "70/70 [==============================] - 0s - loss: 0.2917 - val_loss: 0.2493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.keras.python.keras.callbacks.History at 0x7f251c9494a8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y, epochs=50, batch_size=5,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    4.,    5.,    4.,    1.],\n",
       "       [   6.,    4.,   24.,    9.,    4.],\n",
       "       [   4.,    2.,   25.,   12.,    6.],\n",
       "       [  16.,   22.,   51.,   -5.,   49.],\n",
       "       [   4.,    3.,   33.,   15.,    3.],\n",
       "       [   5.,    7.,   81.,   23.,    4.],\n",
       "       [   2.,    2.,   16.,   13.,    6.],\n",
       "       [  19.,    0.,  124.,   21.,   91.],\n",
       "       [   6.,   10.,   67.,   21.,    2.],\n",
       "       [  23.,    4.,  289.,   39.,  155.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(scaler.inverse_transform(model.predict(X[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    4.,    0.,    0.,    0.],\n",
       "       [   7.,    4.,   10.,    1.,    0.],\n",
       "       [   1.,    0.,    0.,    0.,    0.],\n",
       "       [  13.,   16.,   48.,    3.,   33.],\n",
       "       [   3.,    2.,   25.,   15.,    0.],\n",
       "       [   4.,    7.,  100.,   27.,    0.],\n",
       "       [   0.,    0.,    4.,   15.,    0.],\n",
       "       [  15.,    0.,   85.,   18.,   59.],\n",
       "       [   5.,   10.,   66.,   24.,    0.],\n",
       "       [  28.,    4.,  338.,   47.,  189.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(scaler.inverse_transform(Y[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70 samples, validate on 30 samples\n",
      "Epoch 1/250\n",
      "70/70 [==============================] - 0s - loss: 0.3287 - val_loss: 0.2900\n",
      "Epoch 2/250\n",
      "70/70 [==============================] - 0s - loss: 0.2979 - val_loss: 0.2661\n",
      "Epoch 3/250\n",
      "70/70 [==============================] - 0s - loss: 0.2532 - val_loss: 0.1893\n",
      "Epoch 4/250\n",
      "70/70 [==============================] - 0s - loss: 0.3035 - val_loss: 0.2055\n",
      "Epoch 5/250\n",
      "70/70 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2407\n",
      "Epoch 6/250\n",
      "70/70 [==============================] - 0s - loss: 0.2994 - val_loss: 0.2268\n",
      "Epoch 7/250\n",
      "70/70 [==============================] - 0s - loss: 0.2332 - val_loss: 0.2139\n",
      "Epoch 8/250\n",
      "70/70 [==============================] - 0s - loss: 0.2736 - val_loss: 0.2033\n",
      "Epoch 9/250\n",
      "70/70 [==============================] - 0s - loss: 0.2714 - val_loss: 0.2783\n",
      "Epoch 10/250\n",
      "70/70 [==============================] - 0s - loss: 0.2463 - val_loss: 0.2537\n",
      "Epoch 11/250\n",
      "70/70 [==============================] - 0s - loss: 0.2578 - val_loss: 0.2280\n",
      "Epoch 12/250\n",
      "70/70 [==============================] - 0s - loss: 0.2527 - val_loss: 0.3213\n",
      "Epoch 13/250\n",
      "70/70 [==============================] - 0s - loss: 0.2548 - val_loss: 0.3263\n",
      "Epoch 14/250\n",
      "70/70 [==============================] - 0s - loss: 0.2478 - val_loss: 0.2514\n",
      "Epoch 15/250\n",
      "70/70 [==============================] - 0s - loss: 0.2693 - val_loss: 0.2274\n",
      "Epoch 16/250\n",
      "70/70 [==============================] - 0s - loss: 0.2548 - val_loss: 0.2247\n",
      "Epoch 17/250\n",
      "70/70 [==============================] - 0s - loss: 0.2449 - val_loss: 0.1837\n",
      "Epoch 18/250\n",
      "70/70 [==============================] - 0s - loss: 0.2615 - val_loss: 0.2406\n",
      "Epoch 19/250\n",
      "70/70 [==============================] - 0s - loss: 0.2544 - val_loss: 0.3608\n",
      "Epoch 20/250\n",
      "70/70 [==============================] - 0s - loss: 0.3174 - val_loss: 0.2756\n",
      "Epoch 21/250\n",
      "70/70 [==============================] - 0s - loss: 0.2777 - val_loss: 0.2484\n",
      "Epoch 22/250\n",
      "70/70 [==============================] - 0s - loss: 0.2785 - val_loss: 0.3230\n",
      "Epoch 23/250\n",
      "70/70 [==============================] - 0s - loss: 0.2417 - val_loss: 0.2926\n",
      "Epoch 24/250\n",
      "70/70 [==============================] - 0s - loss: 0.2572 - val_loss: 0.2099\n",
      "Epoch 25/250\n",
      "70/70 [==============================] - 0s - loss: 0.2591 - val_loss: 0.1972\n",
      "Epoch 26/250\n",
      "70/70 [==============================] - 0s - loss: 0.2697 - val_loss: 0.1872\n",
      "Epoch 27/250\n",
      "70/70 [==============================] - 0s - loss: 0.2097 - val_loss: 0.1983\n",
      "Epoch 28/250\n",
      "70/70 [==============================] - 0s - loss: 0.2634 - val_loss: 0.2119\n",
      "Epoch 29/250\n",
      "70/70 [==============================] - 0s - loss: 0.2303 - val_loss: 0.2808\n",
      "Epoch 30/250\n",
      "70/70 [==============================] - 0s - loss: 0.2331 - val_loss: 0.3220\n",
      "Epoch 31/250\n",
      "70/70 [==============================] - 0s - loss: 0.2428 - val_loss: 0.2248\n",
      "Epoch 32/250\n",
      "70/70 [==============================] - 0s - loss: 0.2504 - val_loss: 0.1684\n",
      "Epoch 33/250\n",
      "70/70 [==============================] - 0s - loss: 0.2393 - val_loss: 0.1777\n",
      "Epoch 34/250\n",
      "70/70 [==============================] - 0s - loss: 0.2895 - val_loss: 0.1805ss: 0.290\n",
      "Epoch 35/250\n",
      "70/70 [==============================] - 0s - loss: 0.2282 - val_loss: 0.2474\n",
      "Epoch 36/250\n",
      "70/70 [==============================] - 0s - loss: 0.3466 - val_loss: 0.1987\n",
      "Epoch 37/250\n",
      "70/70 [==============================] - 0s - loss: 0.2559 - val_loss: 0.2250\n",
      "Epoch 38/250\n",
      "70/70 [==============================] - 0s - loss: 0.2490 - val_loss: 0.2174\n",
      "Epoch 39/250\n",
      "70/70 [==============================] - 0s - loss: 0.2400 - val_loss: 0.2153\n",
      "Epoch 40/250\n",
      "70/70 [==============================] - 0s - loss: 0.2359 - val_loss: 0.1779\n",
      "Epoch 41/250\n",
      "70/70 [==============================] - 0s - loss: 0.2510 - val_loss: 0.2123\n",
      "Epoch 42/250\n",
      "70/70 [==============================] - 0s - loss: 0.2280 - val_loss: 0.2965\n",
      "Epoch 43/250\n",
      "70/70 [==============================] - 0s - loss: 0.2956 - val_loss: 0.2556\n",
      "Epoch 44/250\n",
      "70/70 [==============================] - 0s - loss: 0.2608 - val_loss: 0.2263\n",
      "Epoch 45/250\n",
      "70/70 [==============================] - 0s - loss: 0.2135 - val_loss: 0.1817\n",
      "Epoch 46/250\n",
      "70/70 [==============================] - 0s - loss: 0.2549 - val_loss: 0.2617\n",
      "Epoch 47/250\n",
      "70/70 [==============================] - 0s - loss: 0.2300 - val_loss: 0.3273\n",
      "Epoch 48/250\n",
      "70/70 [==============================] - 0s - loss: 0.2432 - val_loss: 0.3412\n",
      "Epoch 49/250\n",
      "70/70 [==============================] - 0s - loss: 0.2279 - val_loss: 0.2857\n",
      "Epoch 50/250\n",
      "70/70 [==============================] - 0s - loss: 0.2876 - val_loss: 0.2298\n",
      "Epoch 51/250\n",
      "70/70 [==============================] - 0s - loss: 0.2816 - val_loss: 0.3261\n",
      "Epoch 52/250\n",
      "70/70 [==============================] - 0s - loss: 0.2531 - val_loss: 0.2242\n",
      "Epoch 53/250\n",
      "70/70 [==============================] - 0s - loss: 0.2434 - val_loss: 0.2153\n",
      "Epoch 54/250\n",
      "70/70 [==============================] - 0s - loss: 0.2734 - val_loss: 0.2219\n",
      "Epoch 55/250\n",
      "70/70 [==============================] - 0s - loss: 0.2549 - val_loss: 0.2058\n",
      "Epoch 56/250\n",
      "70/70 [==============================] - 0s - loss: 0.2127 - val_loss: 0.2169\n",
      "Epoch 57/250\n",
      "70/70 [==============================] - 0s - loss: 0.2100 - val_loss: 0.2757\n",
      "Epoch 58/250\n",
      "70/70 [==============================] - 0s - loss: 0.2497 - val_loss: 0.2053\n",
      "Epoch 59/250\n",
      "70/70 [==============================] - 0s - loss: 0.2133 - val_loss: 0.3115\n",
      "Epoch 60/250\n",
      "70/70 [==============================] - 0s - loss: 0.2278 - val_loss: 0.2886\n",
      "Epoch 61/250\n",
      "70/70 [==============================] - 0s - loss: 0.2247 - val_loss: 0.2103\n",
      "Epoch 62/250\n",
      "70/70 [==============================] - 0s - loss: 0.2659 - val_loss: 0.2820\n",
      "Epoch 63/250\n",
      "70/70 [==============================] - 0s - loss: 0.2584 - val_loss: 0.2993\n",
      "Epoch 64/250\n",
      "70/70 [==============================] - 0s - loss: 0.1675 - val_loss: 0.2607\n",
      "Epoch 65/250\n",
      "70/70 [==============================] - 0s - loss: 0.2404 - val_loss: 0.2069\n",
      "Epoch 66/250\n",
      "70/70 [==============================] - 0s - loss: 0.2257 - val_loss: 0.1714\n",
      "Epoch 67/250\n",
      "70/70 [==============================] - 0s - loss: 0.1754 - val_loss: 0.2147\n",
      "Epoch 68/250\n",
      "70/70 [==============================] - 0s - loss: 0.2196 - val_loss: 0.1817\n",
      "Epoch 69/250\n",
      "70/70 [==============================] - 0s - loss: 0.1917 - val_loss: 0.2431\n",
      "Epoch 70/250\n",
      "70/70 [==============================] - 0s - loss: 0.2562 - val_loss: 0.2120\n",
      "Epoch 71/250\n",
      "70/70 [==============================] - 0s - loss: 0.2531 - val_loss: 0.1717\n",
      "Epoch 72/250\n",
      "70/70 [==============================] - 0s - loss: 0.2378 - val_loss: 0.2490\n",
      "Epoch 73/250\n",
      "70/70 [==============================] - 0s - loss: 0.1918 - val_loss: 0.1990\n",
      "Epoch 74/250\n",
      "70/70 [==============================] - 0s - loss: 0.2419 - val_loss: 0.2042\n",
      "Epoch 75/250\n",
      "70/70 [==============================] - 0s - loss: 0.2119 - val_loss: 0.2294\n",
      "Epoch 76/250\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.220 - 0s - loss: 0.2277 - val_loss: 0.1960\n",
      "Epoch 77/250\n",
      "70/70 [==============================] - 0s - loss: 0.2079 - val_loss: 0.1625\n",
      "Epoch 78/250\n",
      "70/70 [==============================] - 0s - loss: 0.2493 - val_loss: 0.1860\n",
      "Epoch 79/250\n",
      "70/70 [==============================] - 0s - loss: 0.2207 - val_loss: 0.1688\n",
      "Epoch 80/250\n",
      "70/70 [==============================] - 0s - loss: 0.2392 - val_loss: 0.1814\n",
      "Epoch 81/250\n",
      "70/70 [==============================] - 0s - loss: 0.1757 - val_loss: 0.1529\n",
      "Epoch 82/250\n",
      "70/70 [==============================] - 0s - loss: 0.2231 - val_loss: 0.1747\n",
      "Epoch 83/250\n",
      "70/70 [==============================] - 0s - loss: 0.1709 - val_loss: 0.2270\n",
      "Epoch 84/250\n",
      "70/70 [==============================] - 0s - loss: 0.2011 - val_loss: 0.3070\n",
      "Epoch 85/250\n",
      "70/70 [==============================] - 0s - loss: 0.1958 - val_loss: 0.2712\n",
      "Epoch 86/250\n",
      "70/70 [==============================] - 0s - loss: 0.2113 - val_loss: 0.2317\n",
      "Epoch 87/250\n",
      "70/70 [==============================] - 0s - loss: 0.2211 - val_loss: 0.1939\n",
      "Epoch 88/250\n",
      "70/70 [==============================] - 0s - loss: 0.2586 - val_loss: 0.1846\n",
      "Epoch 89/250\n",
      "70/70 [==============================] - 0s - loss: 0.2250 - val_loss: 0.1856\n",
      "Epoch 90/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s - loss: 0.2406 - val_loss: 0.1720\n",
      "Epoch 91/250\n",
      "70/70 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2555\n",
      "Epoch 92/250\n",
      "70/70 [==============================] - 0s - loss: 0.2226 - val_loss: 0.1971\n",
      "Epoch 93/250\n",
      "70/70 [==============================] - 0s - loss: 0.2187 - val_loss: 0.2262\n",
      "Epoch 94/250\n",
      "70/70 [==============================] - 0s - loss: 0.2115 - val_loss: 0.2859\n",
      "Epoch 95/250\n",
      "70/70 [==============================] - 0s - loss: 0.2274 - val_loss: 0.2280\n",
      "Epoch 96/250\n",
      "70/70 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2857\n",
      "Epoch 97/250\n",
      "70/70 [==============================] - 0s - loss: 0.2395 - val_loss: 0.1813\n",
      "Epoch 98/250\n",
      "70/70 [==============================] - 0s - loss: 0.3000 - val_loss: 0.2128\n",
      "Epoch 99/250\n",
      "70/70 [==============================] - 0s - loss: 0.1739 - val_loss: 0.2032\n",
      "Epoch 100/250\n",
      "70/70 [==============================] - 0s - loss: 0.2524 - val_loss: 0.1673\n",
      "Epoch 101/250\n",
      "70/70 [==============================] - 0s - loss: 0.2048 - val_loss: 0.2557\n",
      "Epoch 102/250\n",
      "70/70 [==============================] - 0s - loss: 0.2095 - val_loss: 0.1655\n",
      "Epoch 103/250\n",
      "70/70 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2269\n",
      "Epoch 104/250\n",
      "70/70 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2765\n",
      "Epoch 105/250\n",
      "70/70 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2386\n",
      "Epoch 106/250\n",
      "70/70 [==============================] - 0s - loss: 0.2050 - val_loss: 0.1998\n",
      "Epoch 107/250\n",
      "70/70 [==============================] - 0s - loss: 0.2363 - val_loss: 0.1779\n",
      "Epoch 108/250\n",
      "70/70 [==============================] - 0s - loss: 0.1963 - val_loss: 0.1885\n",
      "Epoch 109/250\n",
      "70/70 [==============================] - 0s - loss: 0.1824 - val_loss: 0.1606\n",
      "Epoch 110/250\n",
      "70/70 [==============================] - 0s - loss: 0.1997 - val_loss: 0.1571\n",
      "Epoch 111/250\n",
      "70/70 [==============================] - 0s - loss: 0.2466 - val_loss: 0.1758\n",
      "Epoch 112/250\n",
      "70/70 [==============================] - 0s - loss: 0.1923 - val_loss: 0.1711\n",
      "Epoch 113/250\n",
      "70/70 [==============================] - 0s - loss: 0.2149 - val_loss: 0.1906\n",
      "Epoch 114/250\n",
      "70/70 [==============================] - 0s - loss: 0.2193 - val_loss: 0.1726\n",
      "Epoch 115/250\n",
      "70/70 [==============================] - 0s - loss: 0.2021 - val_loss: 0.1660\n",
      "Epoch 116/250\n",
      "70/70 [==============================] - 0s - loss: 0.1938 - val_loss: 0.1877\n",
      "Epoch 117/250\n",
      "70/70 [==============================] - 0s - loss: 0.2278 - val_loss: 0.1764\n",
      "Epoch 118/250\n",
      "70/70 [==============================] - 0s - loss: 0.2636 - val_loss: 0.2775\n",
      "Epoch 119/250\n",
      "70/70 [==============================] - 0s - loss: 0.2248 - val_loss: 0.1867\n",
      "Epoch 120/250\n",
      "70/70 [==============================] - 0s - loss: 0.2091 - val_loss: 0.1804\n",
      "Epoch 121/250\n",
      "70/70 [==============================] - 0s - loss: 0.2247 - val_loss: 0.1662\n",
      "Epoch 122/250\n",
      "70/70 [==============================] - 0s - loss: 0.2002 - val_loss: 0.1558\n",
      "Epoch 123/250\n",
      "70/70 [==============================] - 0s - loss: 0.1645 - val_loss: 0.1902\n",
      "Epoch 124/250\n",
      "70/70 [==============================] - 0s - loss: 0.2101 - val_loss: 0.2160\n",
      "Epoch 125/250\n",
      "70/70 [==============================] - 0s - loss: 0.2375 - val_loss: 0.2284\n",
      "Epoch 126/250\n",
      "70/70 [==============================] - 0s - loss: 0.2455 - val_loss: 0.2190\n",
      "Epoch 127/250\n",
      "70/70 [==============================] - 0s - loss: 0.1926 - val_loss: 0.1665\n",
      "Epoch 128/250\n",
      "70/70 [==============================] - 0s - loss: 0.1951 - val_loss: 0.1765\n",
      "Epoch 129/250\n",
      "70/70 [==============================] - 0s - loss: 0.2034 - val_loss: 0.1526\n",
      "Epoch 130/250\n",
      "70/70 [==============================] - 0s - loss: 0.2243 - val_loss: 0.1605\n",
      "Epoch 131/250\n",
      "70/70 [==============================] - 0s - loss: 0.1944 - val_loss: 0.2033\n",
      "Epoch 132/250\n",
      "70/70 [==============================] - 0s - loss: 0.2316 - val_loss: 0.2179\n",
      "Epoch 133/250\n",
      "70/70 [==============================] - 0s - loss: 0.2063 - val_loss: 0.1569\n",
      "Epoch 134/250\n",
      "70/70 [==============================] - 0s - loss: 0.2166 - val_loss: 0.1678\n",
      "Epoch 135/250\n",
      "70/70 [==============================] - 0s - loss: 0.1968 - val_loss: 0.1848\n",
      "Epoch 136/250\n",
      "70/70 [==============================] - 0s - loss: 0.2173 - val_loss: 0.1718\n",
      "Epoch 137/250\n",
      "70/70 [==============================] - 0s - loss: 0.1972 - val_loss: 0.1980\n",
      "Epoch 138/250\n",
      "70/70 [==============================] - 0s - loss: 0.2187 - val_loss: 0.1831\n",
      "Epoch 139/250\n",
      "70/70 [==============================] - 0s - loss: 0.1874 - val_loss: 0.1845\n",
      "Epoch 140/250\n",
      "70/70 [==============================] - 0s - loss: 0.1777 - val_loss: 0.1617\n",
      "Epoch 141/250\n",
      "70/70 [==============================] - 0s - loss: 0.2214 - val_loss: 0.1734\n",
      "Epoch 142/250\n",
      "70/70 [==============================] - 0s - loss: 0.2011 - val_loss: 0.2938\n",
      "Epoch 143/250\n",
      "70/70 [==============================] - 0s - loss: 0.1960 - val_loss: 0.2282\n",
      "Epoch 144/250\n",
      "70/70 [==============================] - 0s - loss: 0.1854 - val_loss: 0.2183\n",
      "Epoch 145/250\n",
      "70/70 [==============================] - 0s - loss: 0.2130 - val_loss: 0.2388\n",
      "Epoch 146/250\n",
      "70/70 [==============================] - 0s - loss: 0.1905 - val_loss: 0.2320\n",
      "Epoch 147/250\n",
      "70/70 [==============================] - 0s - loss: 0.2144 - val_loss: 0.2598\n",
      "Epoch 148/250\n",
      "70/70 [==============================] - 0s - loss: 0.2440 - val_loss: 0.1880\n",
      "Epoch 149/250\n",
      "70/70 [==============================] - 0s - loss: 0.1883 - val_loss: 0.2110\n",
      "Epoch 150/250\n",
      "70/70 [==============================] - 0s - loss: 0.2287 - val_loss: 0.3025\n",
      "Epoch 151/250\n",
      "70/70 [==============================] - 0s - loss: 0.2004 - val_loss: 0.2298\n",
      "Epoch 152/250\n",
      "70/70 [==============================] - 0s - loss: 0.2079 - val_loss: 0.2583\n",
      "Epoch 153/250\n",
      "70/70 [==============================] - 0s - loss: 0.2126 - val_loss: 0.3207\n",
      "Epoch 154/250\n",
      "70/70 [==============================] - 0s - loss: 0.2112 - val_loss: 0.2248\n",
      "Epoch 155/250\n",
      "70/70 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2083\n",
      "Epoch 156/250\n",
      "70/70 [==============================] - 0s - loss: 0.1983 - val_loss: 0.1977\n",
      "Epoch 157/250\n",
      "70/70 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2708\n",
      "Epoch 158/250\n",
      "70/70 [==============================] - 0s - loss: 0.1707 - val_loss: 0.2051\n",
      "Epoch 159/250\n",
      "70/70 [==============================] - 0s - loss: 0.2107 - val_loss: 0.2506\n",
      "Epoch 160/250\n",
      "70/70 [==============================] - 0s - loss: 0.2038 - val_loss: 0.2184\n",
      "Epoch 161/250\n",
      "70/70 [==============================] - 0s - loss: 0.1799 - val_loss: 0.2167\n",
      "Epoch 162/250\n",
      "70/70 [==============================] - 0s - loss: 0.2119 - val_loss: 0.1966\n",
      "Epoch 163/250\n",
      "70/70 [==============================] - 0s - loss: 0.2075 - val_loss: 0.2138\n",
      "Epoch 164/250\n",
      "70/70 [==============================] - 0s - loss: 0.2182 - val_loss: 0.2713\n",
      "Epoch 165/250\n",
      "70/70 [==============================] - 0s - loss: 0.1736 - val_loss: 0.2550\n",
      "Epoch 166/250\n",
      "70/70 [==============================] - 0s - loss: 0.2115 - val_loss: 0.2082\n",
      "Epoch 167/250\n",
      "70/70 [==============================] - 0s - loss: 0.2286 - val_loss: 0.1817\n",
      "Epoch 168/250\n",
      "70/70 [==============================] - 0s - loss: 0.2012 - val_loss: 0.2322\n",
      "Epoch 169/250\n",
      "70/70 [==============================] - 0s - loss: 0.2290 - val_loss: 0.2174\n",
      "Epoch 170/250\n",
      "70/70 [==============================] - 0s - loss: 0.2297 - val_loss: 0.1852\n",
      "Epoch 171/250\n",
      "70/70 [==============================] - 0s - loss: 0.1915 - val_loss: 0.3352\n",
      "Epoch 172/250\n",
      "70/70 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2707\n",
      "Epoch 173/250\n",
      "70/70 [==============================] - 0s - loss: 0.2058 - val_loss: 0.1960\n",
      "Epoch 174/250\n",
      "70/70 [==============================] - 0s - loss: 0.2079 - val_loss: 0.2194\n",
      "Epoch 175/250\n",
      "70/70 [==============================] - 0s - loss: 0.2052 - val_loss: 0.1994\n",
      "Epoch 176/250\n",
      "70/70 [==============================] - 0s - loss: 0.1924 - val_loss: 0.1879\n",
      "Epoch 177/250\n",
      "70/70 [==============================] - 0s - loss: 0.2134 - val_loss: 0.2736\n",
      "Epoch 178/250\n",
      "70/70 [==============================] - 0s - loss: 0.2141 - val_loss: 0.2255\n",
      "Epoch 179/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s - loss: 0.1888 - val_loss: 0.1889\n",
      "Epoch 180/250\n",
      "70/70 [==============================] - 0s - loss: 0.1818 - val_loss: 0.1873\n",
      "Epoch 181/250\n",
      "70/70 [==============================] - 0s - loss: 0.2240 - val_loss: 0.1823\n",
      "Epoch 182/250\n",
      "70/70 [==============================] - 0s - loss: 0.2028 - val_loss: 0.2209\n",
      "Epoch 183/250\n",
      "70/70 [==============================] - 0s - loss: 0.2198 - val_loss: 0.1867\n",
      "Epoch 184/250\n",
      "70/70 [==============================] - 0s - loss: 0.1911 - val_loss: 0.1964\n",
      "Epoch 185/250\n",
      "70/70 [==============================] - 0s - loss: 0.1892 - val_loss: 0.2903\n",
      "Epoch 186/250\n",
      "70/70 [==============================] - 0s - loss: 0.1812 - val_loss: 0.2553\n",
      "Epoch 187/250\n",
      "70/70 [==============================] - 0s - loss: 0.1713 - val_loss: 0.2018\n",
      "Epoch 188/250\n",
      "70/70 [==============================] - 0s - loss: 0.1648 - val_loss: 0.2336\n",
      "Epoch 189/250\n",
      "70/70 [==============================] - 0s - loss: 0.2070 - val_loss: 0.2341\n",
      "Epoch 190/250\n",
      "70/70 [==============================] - 0s - loss: 0.1779 - val_loss: 0.2610\n",
      "Epoch 191/250\n",
      "70/70 [==============================] - 0s - loss: 0.2015 - val_loss: 0.2111\n",
      "Epoch 192/250\n",
      "70/70 [==============================] - 0s - loss: 0.2253 - val_loss: 0.1816\n",
      "Epoch 193/250\n",
      "70/70 [==============================] - 0s - loss: 0.1941 - val_loss: 0.2136\n",
      "Epoch 194/250\n",
      "70/70 [==============================] - 0s - loss: 0.2223 - val_loss: 0.1912\n",
      "Epoch 195/250\n",
      "70/70 [==============================] - 0s - loss: 0.1618 - val_loss: 0.1774\n",
      "Epoch 196/250\n",
      "70/70 [==============================] - 0s - loss: 0.1968 - val_loss: 0.1916\n",
      "Epoch 197/250\n",
      "70/70 [==============================] - 0s - loss: 0.1768 - val_loss: 0.2340\n",
      "Epoch 198/250\n",
      "70/70 [==============================] - 0s - loss: 0.2058 - val_loss: 0.2123\n",
      "Epoch 199/250\n",
      "70/70 [==============================] - 0s - loss: 0.1830 - val_loss: 0.1952\n",
      "Epoch 200/250\n",
      "70/70 [==============================] - 0s - loss: 0.1954 - val_loss: 0.2197\n",
      "Epoch 201/250\n",
      "70/70 [==============================] - 0s - loss: 0.2377 - val_loss: 0.2456\n",
      "Epoch 202/250\n",
      "70/70 [==============================] - 0s - loss: 0.1937 - val_loss: 0.1871\n",
      "Epoch 203/250\n",
      "70/70 [==============================] - 0s - loss: 0.2293 - val_loss: 0.2528\n",
      "Epoch 204/250\n",
      "70/70 [==============================] - 0s - loss: 0.1646 - val_loss: 0.2106\n",
      "Epoch 205/250\n",
      "70/70 [==============================] - 0s - loss: 0.1891 - val_loss: 0.1948\n",
      "Epoch 206/250\n",
      "70/70 [==============================] - 0s - loss: 0.1691 - val_loss: 0.2973\n",
      "Epoch 207/250\n",
      "70/70 [==============================] - 0s - loss: 0.1899 - val_loss: 0.2147\n",
      "Epoch 208/250\n",
      "70/70 [==============================] - 0s - loss: 0.2050 - val_loss: 0.2009\n",
      "Epoch 209/250\n",
      "70/70 [==============================] - 0s - loss: 0.2003 - val_loss: 0.2400\n",
      "Epoch 210/250\n",
      "70/70 [==============================] - 0s - loss: 0.1604 - val_loss: 0.1918\n",
      "Epoch 211/250\n",
      "70/70 [==============================] - 0s - loss: 0.1825 - val_loss: 0.1993\n",
      "Epoch 212/250\n",
      "70/70 [==============================] - 0s - loss: 0.2087 - val_loss: 0.2115\n",
      "Epoch 213/250\n",
      "70/70 [==============================] - 0s - loss: 0.1591 - val_loss: 0.2226\n",
      "Epoch 214/250\n",
      "70/70 [==============================] - 0s - loss: 0.1813 - val_loss: 0.2634\n",
      "Epoch 215/250\n",
      "70/70 [==============================] - 0s - loss: 0.1809 - val_loss: 0.1491\n",
      "Epoch 216/250\n",
      "70/70 [==============================] - 0s - loss: 0.2463 - val_loss: 0.1916\n",
      "Epoch 217/250\n",
      "70/70 [==============================] - 0s - loss: 0.1800 - val_loss: 0.2264\n",
      "Epoch 218/250\n",
      "70/70 [==============================] - 0s - loss: 0.1802 - val_loss: 0.2275\n",
      "Epoch 219/250\n",
      "70/70 [==============================] - 0s - loss: 0.1814 - val_loss: 0.1882\n",
      "Epoch 220/250\n",
      "70/70 [==============================] - 0s - loss: 0.2160 - val_loss: 0.2059\n",
      "Epoch 221/250\n",
      "70/70 [==============================] - 0s - loss: 0.1719 - val_loss: 0.2078\n",
      "Epoch 222/250\n",
      "70/70 [==============================] - 0s - loss: 0.2205 - val_loss: 0.1801\n",
      "Epoch 223/250\n",
      "70/70 [==============================] - 0s - loss: 0.1933 - val_loss: 0.3084\n",
      "Epoch 224/250\n",
      "70/70 [==============================] - 0s - loss: 0.2124 - val_loss: 0.1669\n",
      "Epoch 225/250\n",
      "70/70 [==============================] - 0s - loss: 0.1746 - val_loss: 0.2084\n",
      "Epoch 226/250\n",
      "70/70 [==============================] - 0s - loss: 0.1985 - val_loss: 0.2155\n",
      "Epoch 227/250\n",
      "70/70 [==============================] - 0s - loss: 0.1913 - val_loss: 0.1701\n",
      "Epoch 228/250\n",
      "70/70 [==============================] - 0s - loss: 0.1734 - val_loss: 0.1637\n",
      "Epoch 229/250\n",
      "70/70 [==============================] - 0s - loss: 0.2026 - val_loss: 0.1967\n",
      "Epoch 230/250\n",
      "70/70 [==============================] - 0s - loss: 0.2007 - val_loss: 0.1893\n",
      "Epoch 231/250\n",
      "70/70 [==============================] - 0s - loss: 0.1894 - val_loss: 0.2080\n",
      "Epoch 232/250\n",
      "70/70 [==============================] - 0s - loss: 0.2062 - val_loss: 0.1877\n",
      "Epoch 233/250\n",
      "70/70 [==============================] - 0s - loss: 0.2033 - val_loss: 0.1636\n",
      "Epoch 234/250\n",
      "70/70 [==============================] - 0s - loss: 0.2058 - val_loss: 0.2199\n",
      "Epoch 235/250\n",
      "70/70 [==============================] - 0s - loss: 0.1945 - val_loss: 0.2012\n",
      "Epoch 236/250\n",
      "70/70 [==============================] - 0s - loss: 0.1945 - val_loss: 0.1724\n",
      "Epoch 237/250\n",
      "70/70 [==============================] - 0s - loss: 0.2007 - val_loss: 0.2321\n",
      "Epoch 238/250\n",
      "70/70 [==============================] - 0s - loss: 0.1720 - val_loss: 0.2310\n",
      "Epoch 239/250\n",
      "70/70 [==============================] - 0s - loss: 0.1747 - val_loss: 0.2535\n",
      "Epoch 240/250\n",
      "70/70 [==============================] - 0s - loss: 0.1848 - val_loss: 0.2536\n",
      "Epoch 241/250\n",
      "70/70 [==============================] - 0s - loss: 0.1734 - val_loss: 0.2133\n",
      "Epoch 242/250\n",
      "70/70 [==============================] - 0s - loss: 0.2070 - val_loss: 0.1767\n",
      "Epoch 243/250\n",
      "70/70 [==============================] - 0s - loss: 0.1751 - val_loss: 0.2025\n",
      "Epoch 244/250\n",
      "70/70 [==============================] - 0s - loss: 0.1688 - val_loss: 0.1710\n",
      "Epoch 245/250\n",
      "70/70 [==============================] - 0s - loss: 0.1585 - val_loss: 0.1358\n",
      "Epoch 246/250\n",
      "70/70 [==============================] - 0s - loss: 0.1897 - val_loss: 0.2034\n",
      "Epoch 247/250\n",
      "70/70 [==============================] - 0s - loss: 0.1844 - val_loss: 0.2088\n",
      "Epoch 248/250\n",
      "70/70 [==============================] - 0s - loss: 0.2031 - val_loss: 0.1656\n",
      "Epoch 249/250\n",
      "70/70 [==============================] - 0s - loss: 0.1955 - val_loss: 0.1873\n",
      "Epoch 250/250\n",
      "70/70 [==============================] - 0s - loss: 0.1931 - val_loss: 0.1774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.keras.python.keras.callbacks.History at 0x7f24b67764e0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y, epochs=250, batch_size=5,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2.,    4.,    7.,    3.,    0.],\n",
       "       [   6.,    4.,   19.,    6.,    0.],\n",
       "       [   2.,    1.,    5.,    5.,   -0.],\n",
       "       [  12.,   14.,   50.,    6.,   31.],\n",
       "       [   3.,    2.,   24.,   15.,   -0.],\n",
       "       [   4.,    6.,   69.,   22.,    0.],\n",
       "       [   0.,    0.,    6.,   15.,    0.],\n",
       "       [  14.,    0.,   86.,   18.,   58.],\n",
       "       [   5.,    9.,   58.,   21.,   -0.],\n",
       "       [  27.,    4.,  321.,   46.,  182.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(scaler.inverse_transform(model.predict(X[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    4.,    0.,    0.,    0.],\n",
       "       [   7.,    4.,   10.,    1.,    0.],\n",
       "       [   1.,    0.,    0.,    0.,    0.],\n",
       "       [  13.,   16.,   48.,    3.,   33.],\n",
       "       [   3.,    2.,   25.,   15.,    0.],\n",
       "       [   4.,    7.,  100.,   27.,    0.],\n",
       "       [   0.,    0.,    4.,   15.,    0.],\n",
       "       [  15.,    0.,   85.,   18.,   59.],\n",
       "       [   5.,   10.,   66.,   24.,    0.],\n",
       "       [  28.,    4.,  338.,   47.,  189.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(scaler.inverse_transform(Y[:10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nb35]",
   "language": "python",
   "name": "conda-env-nb35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
